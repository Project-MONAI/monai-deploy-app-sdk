{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Multi-AI Deploy App with Multiple Models\n",
    "\n",
    "This tutorial shows how to create an inference application with multiple models, focusing on model files organization, accessing and inferring with named model network in the application, and finally building an app package.\n",
    "\n",
    "Typically multiple models will work in tandem, e.g. a lung segmentation model's output, along with the original image, are then used by a lung nodule detection and classification model. There are, however, no such models in the [MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo) as of now. So, for illustration purpose, two independent models will be used in this example, [Spleen Segmentation](https://github.com/Project-MONAI/model-zoo/tree/dev/models/spleen_ct_segmentation) and [Pancreas Segmentation](https://github.com/Project-MONAI/model-zoo/tree/dev/models/pancreas_ct_dints_segmentation), both are trained with DICOM images of CT modality, and both are packaged in the [MONAI Bundle](https://docs.monai.io/en/latest/bundle_intro.html) format. A single input of a CT Abdomen DICOM Series can be used for both models within the application.\n",
    "\n",
    "\n",
    "## Important Steps\n",
    "- Place the model TorchScripts in a defined folder structure, see below for details\n",
    "- Pass the model name to the inference operator instance in the app\n",
    "- Connect the input to and output from the inference operators, as required by the app\n",
    "\n",
    "## Required Model File Organization\n",
    "\n",
    "- The model files in TorchScript, be it MONAI Bundle compliant or not, must each be placed in an uniquely named folder. The name of this folder becomes the model network name within the application for it to retrieve the loaded model network from the execution context.\n",
    "- The folders containing the individual model file must then be place under a parent folder. The name of this folder can be any valid folder name chosen by the application developer.\n",
    "- The path of the aforementioned parent folder then needs to be used as the value for the well-known environment variable for model path.\n",
    "\n",
    "## Example Model File Organization\n",
    "\n",
    "In this example, the models are organized as shown below.\n",
    "```\n",
    "multi_models\n",
    "├── pancreas_ct_dints\n",
    "│   └── model.ts\n",
    "└── spleen_ct\n",
    "    └── model.ts\n",
    "```\n",
    "\n",
    "Please note,\n",
    "\n",
    "- The `multi_models` is the parent folder, whose path is used as the value for setting the well-known environment variable for model path, and when using App SDK CLI to build the application package.\n",
    "- The sub-folder names become model network names, `pancreas_ct_dints` and `spleen_model`, respectively.\n",
    "\n",
    "In the following sections, we will demonstrate how to create and package the application using these two models.\n",
    "\n",
    ":::{note}\n",
    "The two models are both MONAI bundles, published in [MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo)\n",
    "- [spleen_ct_segmentation, v0.3.2](https://github.com/Project-MONAI/model-zoo/tree/dev/models/spleen_ct_segmentation)\n",
    "- [pancreas_ct_dints_segmentation, v0.3.0](https://github.com/Project-MONAI/model-zoo/tree/dev/models/pancreas_ct_dints_segmentation)\n",
    "\n",
    "The DICOM CT series used as test input is downloaded from [TCIA](https://www.cancerimagingarchive.net/), CT Abdomen Collection ID `CPTAC-PDA` Subject ID `C3N-00198`.\n",
    "\n",
    "Both the DICOM files and the models have been packaged and shared on Google Drive.\n",
    ":::\n",
    "\n",
    "## Creating Operators and connecting them in Application class\n",
    "\n",
    "We will implement an application that consists of seven Operators:\n",
    "\n",
    "- **DICOMDataLoaderOperator**:\n",
    "    - **Input(dicom_files)**: a folder path (`Path`)\n",
    "    - **Output(dicom_study_list)**: a list of DICOM studies in memory (List[[`DICOMStudy`](/modules/_autosummary/monai.deploy.core.domain.DICOMStudy)])\n",
    "- **DICOMSeriesSelectorOperator**:\n",
    "    - **Input(dicom_study_list)**: a list of DICOM studies in memory (List[[`DICOMStudy`](/modules/_autosummary/monai.deploy.core.domain.DICOMStudy)])\n",
    "    - **Input(selection_rules)**: a selection rule (Dict)\n",
    "    - **Output(study_selected_series_list)**: a DICOM series object in memory ([`StudySelectedSeries`](/modules/_autosummary/monai.deploy.core.domain.StudySelectedSeries))\n",
    "- **DICOMSeriesToVolumeOperator**:\n",
    "    - **Input(study_selected_series_list)**: a DICOM series object in memory ([`StudySelectedSeries`](/modules/_autosummary/monai.deploy.core.domain.StudySelectedSeries))\n",
    "    - **Output(image)**: an image object in memory ([`Image`](/modules/_autosummary/monai.deploy.core.domain.Image))\n",
    "- **MonaiBundleInferenceOperator** x 2:\n",
    "    - **Input(image)**: an image object in memory ([`Image`](/modules/_autosummary/monai.deploy.core.domain.Image))\n",
    "    - **Output(pred)**: an image object in memory ([`Image`](/modules/_autosummary/monai.deploy.core.domain.Image))\n",
    "- **DICOMSegmentationWriterOperator** x2:\n",
    "    - **Input(seg_image)**: a segmentation image object in memory ([`Image`](/modules/_autosummary/monai.deploy.core.domain.Image))\n",
    "    - **Input(study_selected_series_list)**: a DICOM series object in memory ([`StudySelectedSeries`](/modules/_autosummary/monai.deploy.core.domain.StudySelectedSeries))\n",
    "    - **Output(dicom_seg_instance)**: a file path (`Path`)\n",
    "\n",
    "\n",
    ":::{note}\n",
    "The `DICOMSegmentationWriterOperator` needs both the segmentation image as well as the original DICOM series for reusing the patient demographics and other DICOM Study level attributes, as well as referring to the original SOP instance UID.\n",
    ":::\n",
    "\n",
    "The workflow of the application is illustrated below.\n",
    "\n",
    "```{mermaid}\n",
    "%%{init: {\"theme\": \"base\", \"themeVariables\": { \"fontSize\": \"16px\"}} }%%\n",
    "\n",
    "classDiagram\n",
    "    direction TB\n",
    "    DICOMDataLoaderOperator --|> DICOMSeriesSelectorOperator : dicom_study_list...dicom_study_list\n",
    "    DICOMSeriesSelectorOperator --|> DICOMSeriesToVolumeOperator : study_selected_series_list...study_selected_series_list\n",
    "\n",
    "    DICOMSeriesToVolumeOperator --|> Spleen_BundleInferenceOperator : image...image\n",
    "    DICOMSeriesSelectorOperator --|> Spleen_DICOMSegmentationWriterOperator : study_selected_series_list...study_selected_series_list\n",
    "    Spleen_BundleInferenceOperator --|> Spleen_DICOMSegmentationWriterOperator : pred...seg_image\n",
    "\n",
    "    DICOMSeriesToVolumeOperator --|> Pancreas_BundleInferenceOperator : image...image\n",
    "    DICOMSeriesSelectorOperator --|> Pancreas_DICOMSegmentationWriterOperator : study_selected_series_list...study_selected_series_list\n",
    "    Pancreas_BundleInferenceOperator --|> Pancreas_DICOMSegmentationWriterOperator : pred...seg_image\n",
    "\n",
    "    class DICOMDataLoaderOperator {\n",
    "        <in>dicom_files : DISK\n",
    "        dicom_study_list(out) IN_MEMORY\n",
    "    }\n",
    "    class DICOMSeriesSelectorOperator {\n",
    "        <in>dicom_study_list : IN_MEMORY\n",
    "        <in>selection_rules : IN_MEMORY\n",
    "        study_selected_series_list(out) IN_MEMORY\n",
    "    }\n",
    "    class DICOMSeriesToVolumeOperator {\n",
    "        <in>study_selected_series_list : IN_MEMORY\n",
    "        image(out) IN_MEMORY\n",
    "    }\n",
    "    class Spleen_BundleInferenceOperator {\n",
    "        <in>image : IN_MEMORY\n",
    "        pred(out) IN_MEMORY\n",
    "    }\n",
    "    class Pancreas_BundleInferenceOperator {\n",
    "        <in>image : IN_MEMORY\n",
    "        pred(out) IN_MEMORY\n",
    "    }\n",
    "    class Spleen_DICOMSegmentationWriterOperator {\n",
    "        <in>seg_image : IN_MEMORY\n",
    "        <in>study_selected_series_list : IN_MEMORY\n",
    "        dicom_seg_instance(out) DISK\n",
    "    }\n",
    "    class Pancreas_DICOMSegmentationWriterOperator {\n",
    "        <in>seg_image : IN_MEMORY\n",
    "        <in>study_selected_series_list : IN_MEMORY\n",
    "        dicom_seg_instance(out) DISK\n",
    "    }\n",
    "```\n",
    "\n",
    "### Setup environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MONAI and other necessary image processing packages for the application\n",
    "!python -c \"import monai\" || pip install --upgrade -q \"monai\"\n",
    "!python -c \"import torch\" || pip install -q \"torch>=1.12.0\"\n",
    "!python -c \"import numpy\" || pip install -q \"numpy>=1.21\"\n",
    "!python -c \"import nibabel\" || pip install -q \"nibabel>=3.2.1\"\n",
    "!python -c \"import pydicom\" || pip install -q \"pydicom>=1.4.2\"\n",
    "!python -c \"import highdicom\" || pip install -q \"highdicom>=0.18.2\"\n",
    "!python -c \"import SimpleITK\" || pip install -q \"SimpleITK>=2.0.0\"\n",
    "\n",
    "# Install MONAI Deploy App SDK package\n",
    "!python -c \"import monai.deploy\" || pip install -q \"monai-deploy-app-sdk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you may need to restart the Jupyter kernel to use the updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download/Extract input and model/bundle files from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (4.6.4)\n",
      "Requirement already satisfied: filelock in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (from gdown) (3.10.0)\n",
      "Requirement already satisfied: requests[socks] in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (from gdown) (2.28.2)\n",
      "Requirement already satisfied: six in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (from gdown) (4.12.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (from requests[socks]->gdown) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (from requests[socks]->gdown) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (from requests[socks]->gdown) (2022.12.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1llJ4NGNTjY187RLX4MtlmHYhfGxBNWmd\n",
      "To: /home/mqin/src/monai-deploy-app-sdk/notebooks/tutorials/ai_multi_model_bundle_data.zip\n",
      "100%|████████████████████████████████████████| 647M/647M [00:08<00:00, 75.6MB/s]\n",
      "Archive:  ai_multi_model_bundle_data.zip\n",
      "  inflating: dcm/1-001.dcm           \n",
      "  inflating: dcm/1-002.dcm           \n",
      "  inflating: dcm/1-003.dcm           \n",
      "  inflating: dcm/1-004.dcm           \n",
      "  inflating: dcm/1-005.dcm           \n",
      "  inflating: dcm/1-006.dcm           \n",
      "  inflating: dcm/1-007.dcm           \n",
      "  inflating: dcm/1-008.dcm           \n",
      "  inflating: dcm/1-009.dcm           \n",
      "  inflating: dcm/1-010.dcm           \n",
      "  inflating: dcm/1-011.dcm           \n",
      "  inflating: dcm/1-012.dcm           \n",
      "  inflating: dcm/1-013.dcm           \n",
      "  inflating: dcm/1-014.dcm           \n",
      "  inflating: dcm/1-015.dcm           \n",
      "  inflating: dcm/1-016.dcm           \n",
      "  inflating: dcm/1-017.dcm           \n",
      "  inflating: dcm/1-018.dcm           \n",
      "  inflating: dcm/1-019.dcm           \n",
      "  inflating: dcm/1-020.dcm           \n",
      "  inflating: dcm/1-021.dcm           \n",
      "  inflating: dcm/1-022.dcm           \n",
      "  inflating: dcm/1-023.dcm           \n",
      "  inflating: dcm/1-024.dcm           \n",
      "  inflating: dcm/1-025.dcm           \n",
      "  inflating: dcm/1-026.dcm           \n",
      "  inflating: dcm/1-027.dcm           \n",
      "  inflating: dcm/1-028.dcm           \n",
      "  inflating: dcm/1-029.dcm           \n",
      "  inflating: dcm/1-030.dcm           \n",
      "  inflating: dcm/1-031.dcm           \n",
      "  inflating: dcm/1-032.dcm           \n",
      "  inflating: dcm/1-033.dcm           \n",
      "  inflating: dcm/1-034.dcm           \n",
      "  inflating: dcm/1-035.dcm           \n",
      "  inflating: dcm/1-036.dcm           \n",
      "  inflating: dcm/1-037.dcm           \n",
      "  inflating: dcm/1-038.dcm           \n",
      "  inflating: dcm/1-039.dcm           \n",
      "  inflating: dcm/1-040.dcm           \n",
      "  inflating: dcm/1-041.dcm           \n",
      "  inflating: dcm/1-042.dcm           \n",
      "  inflating: dcm/1-043.dcm           \n",
      "  inflating: dcm/1-044.dcm           \n",
      "  inflating: dcm/1-045.dcm           \n",
      "  inflating: dcm/1-046.dcm           \n",
      "  inflating: dcm/1-047.dcm           \n",
      "  inflating: dcm/1-048.dcm           \n",
      "  inflating: dcm/1-049.dcm           \n",
      "  inflating: dcm/1-050.dcm           \n",
      "  inflating: dcm/1-051.dcm           \n",
      "  inflating: dcm/1-052.dcm           \n",
      "  inflating: dcm/1-053.dcm           \n",
      "  inflating: dcm/1-054.dcm           \n",
      "  inflating: dcm/1-055.dcm           \n",
      "  inflating: dcm/1-056.dcm           \n",
      "  inflating: dcm/1-057.dcm           \n",
      "  inflating: dcm/1-058.dcm           \n",
      "  inflating: dcm/1-059.dcm           \n",
      "  inflating: dcm/1-060.dcm           \n",
      "  inflating: dcm/1-061.dcm           \n",
      "  inflating: dcm/1-062.dcm           \n",
      "  inflating: dcm/1-063.dcm           \n",
      "  inflating: dcm/1-064.dcm           \n",
      "  inflating: dcm/1-065.dcm           \n",
      "  inflating: dcm/1-066.dcm           \n",
      "  inflating: dcm/1-067.dcm           \n",
      "  inflating: dcm/1-068.dcm           \n",
      "  inflating: dcm/1-069.dcm           \n",
      "  inflating: dcm/1-070.dcm           \n",
      "  inflating: dcm/1-071.dcm           \n",
      "  inflating: dcm/1-072.dcm           \n",
      "  inflating: dcm/1-073.dcm           \n",
      "  inflating: dcm/1-074.dcm           \n",
      "  inflating: dcm/1-075.dcm           \n",
      "  inflating: dcm/1-076.dcm           \n",
      "  inflating: dcm/1-077.dcm           \n",
      "  inflating: dcm/1-078.dcm           \n",
      "  inflating: dcm/1-079.dcm           \n",
      "  inflating: dcm/1-080.dcm           \n",
      "  inflating: dcm/1-081.dcm           \n",
      "  inflating: dcm/1-082.dcm           \n",
      "  inflating: dcm/1-083.dcm           \n",
      "  inflating: dcm/1-084.dcm           \n",
      "  inflating: dcm/1-085.dcm           \n",
      "  inflating: dcm/1-086.dcm           \n",
      "  inflating: dcm/1-087.dcm           \n",
      "  inflating: dcm/1-088.dcm           \n",
      "  inflating: dcm/1-089.dcm           \n",
      "  inflating: dcm/1-090.dcm           \n",
      "  inflating: dcm/1-091.dcm           \n",
      "  inflating: dcm/1-092.dcm           \n",
      "  inflating: dcm/1-093.dcm           \n",
      "  inflating: dcm/1-094.dcm           \n",
      "  inflating: dcm/1-095.dcm           \n",
      "  inflating: dcm/1-096.dcm           \n",
      "  inflating: dcm/1-097.dcm           \n",
      "  inflating: dcm/1-098.dcm           \n",
      "  inflating: dcm/1-099.dcm           \n",
      "  inflating: dcm/1-100.dcm           \n",
      "  inflating: dcm/1-101.dcm           \n",
      "  inflating: dcm/1-102.dcm           \n",
      "  inflating: dcm/1-103.dcm           \n",
      "  inflating: dcm/1-104.dcm           \n",
      "  inflating: dcm/1-105.dcm           \n",
      "  inflating: dcm/1-106.dcm           \n",
      "  inflating: dcm/1-107.dcm           \n",
      "  inflating: dcm/1-108.dcm           \n",
      "  inflating: dcm/1-109.dcm           \n",
      "  inflating: dcm/1-110.dcm           \n",
      "  inflating: dcm/1-111.dcm           \n",
      "  inflating: dcm/1-112.dcm           \n",
      "  inflating: dcm/1-113.dcm           \n",
      "  inflating: dcm/1-114.dcm           \n",
      "  inflating: dcm/1-115.dcm           \n",
      "  inflating: dcm/1-116.dcm           \n",
      "  inflating: dcm/1-117.dcm           \n",
      "  inflating: dcm/1-118.dcm           \n",
      "  inflating: dcm/1-119.dcm           \n",
      "  inflating: dcm/1-120.dcm           \n",
      "  inflating: dcm/1-121.dcm           \n",
      "  inflating: dcm/1-122.dcm           \n",
      "  inflating: dcm/1-123.dcm           \n",
      "  inflating: dcm/1-124.dcm           \n",
      "  inflating: dcm/1-125.dcm           \n",
      "  inflating: dcm/1-126.dcm           \n",
      "  inflating: dcm/1-127.dcm           \n",
      "  inflating: dcm/1-128.dcm           \n",
      "  inflating: dcm/1-129.dcm           \n",
      "  inflating: dcm/1-130.dcm           \n",
      "  inflating: dcm/1-131.dcm           \n",
      "  inflating: dcm/1-132.dcm           \n",
      "  inflating: dcm/1-133.dcm           \n",
      "  inflating: dcm/1-134.dcm           \n",
      "  inflating: dcm/1-135.dcm           \n",
      "  inflating: dcm/1-136.dcm           \n",
      "  inflating: dcm/1-137.dcm           \n",
      "  inflating: dcm/1-138.dcm           \n",
      "  inflating: dcm/1-139.dcm           \n",
      "  inflating: dcm/1-140.dcm           \n",
      "  inflating: dcm/1-141.dcm           \n",
      "  inflating: dcm/1-142.dcm           \n",
      "  inflating: dcm/1-143.dcm           \n",
      "  inflating: dcm/1-144.dcm           \n",
      "  inflating: dcm/1-145.dcm           \n",
      "  inflating: dcm/1-146.dcm           \n",
      "  inflating: dcm/1-147.dcm           \n",
      "  inflating: dcm/1-148.dcm           \n",
      "  inflating: dcm/1-149.dcm           \n",
      "  inflating: dcm/1-150.dcm           \n",
      "  inflating: dcm/1-151.dcm           \n",
      "  inflating: dcm/1-152.dcm           \n",
      "  inflating: dcm/1-153.dcm           \n",
      "  inflating: dcm/1-154.dcm           \n",
      "  inflating: dcm/1-155.dcm           \n",
      "  inflating: dcm/1-156.dcm           \n",
      "  inflating: dcm/1-157.dcm           \n",
      "  inflating: dcm/1-158.dcm           \n",
      "  inflating: dcm/1-159.dcm           \n",
      "  inflating: dcm/1-160.dcm           \n",
      "  inflating: dcm/1-161.dcm           \n",
      "  inflating: dcm/1-162.dcm           \n",
      "  inflating: dcm/1-163.dcm           \n",
      "  inflating: dcm/1-164.dcm           \n",
      "  inflating: dcm/1-165.dcm           \n",
      "  inflating: dcm/1-166.dcm           \n",
      "  inflating: dcm/1-167.dcm           \n",
      "  inflating: dcm/1-168.dcm           \n",
      "  inflating: dcm/1-169.dcm           \n",
      "  inflating: dcm/1-170.dcm           \n",
      "  inflating: dcm/1-171.dcm           \n",
      "  inflating: dcm/1-172.dcm           \n",
      "  inflating: dcm/1-173.dcm           \n",
      "  inflating: dcm/1-174.dcm           \n",
      "  inflating: dcm/1-175.dcm           \n",
      "  inflating: dcm/1-176.dcm           \n",
      "  inflating: dcm/1-177.dcm           \n",
      "  inflating: dcm/1-178.dcm           \n",
      "  inflating: dcm/1-179.dcm           \n",
      "  inflating: dcm/1-180.dcm           \n",
      "  inflating: dcm/1-181.dcm           \n",
      "  inflating: dcm/1-182.dcm           \n",
      "  inflating: dcm/1-183.dcm           \n",
      "  inflating: dcm/1-184.dcm           \n",
      "  inflating: dcm/1-185.dcm           \n",
      "  inflating: dcm/1-186.dcm           \n",
      "  inflating: dcm/1-187.dcm           \n",
      "  inflating: dcm/1-188.dcm           \n",
      "  inflating: dcm/1-189.dcm           \n",
      "  inflating: dcm/1-190.dcm           \n",
      "  inflating: dcm/1-191.dcm           \n",
      "  inflating: dcm/1-192.dcm           \n",
      "  inflating: dcm/1-193.dcm           \n",
      "  inflating: dcm/1-194.dcm           \n",
      "  inflating: dcm/1-195.dcm           \n",
      "  inflating: dcm/1-196.dcm           \n",
      "  inflating: dcm/1-197.dcm           \n",
      "  inflating: dcm/1-198.dcm           \n",
      "  inflating: dcm/1-199.dcm           \n",
      "  inflating: dcm/1-200.dcm           \n",
      "  inflating: dcm/1-201.dcm           \n",
      "  inflating: dcm/1-202.dcm           \n",
      "  inflating: dcm/1-203.dcm           \n",
      "  inflating: dcm/1-204.dcm           \n",
      "  inflating: multi_models/pancreas_ct_dints/model.ts  \n",
      "  inflating: multi_models/spleen_ct/model.ts  \n"
     ]
    }
   ],
   "source": [
    "# Download the test data and MONAI bundle zip file\n",
    "!pip install gdown \n",
    "!gdown \"https://drive.google.com/uc?id=1llJ4NGNTjY187RLX4MtlmHYhfGxBNWmd\"\n",
    "\n",
    "# After downloading ai_spleen_bundle_data zip file from the web browser or using gdown,\n",
    "!unzip -o \"ai_multi_model_bundle_data.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HOLOSCAN_INPUT_PATH=dcm\n",
      "env: HOLOSCAN_MODEL_PATH=multi_models\n",
      "env: HOLOSCAN_OUTPUT_PATH=output\n",
      "1-001.dcm  1-031.dcm  1-061.dcm  1-091.dcm  1-121.dcm  1-151.dcm  1-181.dcm\n",
      "1-002.dcm  1-032.dcm  1-062.dcm  1-092.dcm  1-122.dcm  1-152.dcm  1-182.dcm\n",
      "1-003.dcm  1-033.dcm  1-063.dcm  1-093.dcm  1-123.dcm  1-153.dcm  1-183.dcm\n",
      "1-004.dcm  1-034.dcm  1-064.dcm  1-094.dcm  1-124.dcm  1-154.dcm  1-184.dcm\n",
      "1-005.dcm  1-035.dcm  1-065.dcm  1-095.dcm  1-125.dcm  1-155.dcm  1-185.dcm\n",
      "1-006.dcm  1-036.dcm  1-066.dcm  1-096.dcm  1-126.dcm  1-156.dcm  1-186.dcm\n",
      "1-007.dcm  1-037.dcm  1-067.dcm  1-097.dcm  1-127.dcm  1-157.dcm  1-187.dcm\n",
      "1-008.dcm  1-038.dcm  1-068.dcm  1-098.dcm  1-128.dcm  1-158.dcm  1-188.dcm\n",
      "1-009.dcm  1-039.dcm  1-069.dcm  1-099.dcm  1-129.dcm  1-159.dcm  1-189.dcm\n",
      "1-010.dcm  1-040.dcm  1-070.dcm  1-100.dcm  1-130.dcm  1-160.dcm  1-190.dcm\n",
      "1-011.dcm  1-041.dcm  1-071.dcm  1-101.dcm  1-131.dcm  1-161.dcm  1-191.dcm\n",
      "1-012.dcm  1-042.dcm  1-072.dcm  1-102.dcm  1-132.dcm  1-162.dcm  1-192.dcm\n",
      "1-013.dcm  1-043.dcm  1-073.dcm  1-103.dcm  1-133.dcm  1-163.dcm  1-193.dcm\n",
      "1-014.dcm  1-044.dcm  1-074.dcm  1-104.dcm  1-134.dcm  1-164.dcm  1-194.dcm\n",
      "1-015.dcm  1-045.dcm  1-075.dcm  1-105.dcm  1-135.dcm  1-165.dcm  1-195.dcm\n",
      "1-016.dcm  1-046.dcm  1-076.dcm  1-106.dcm  1-136.dcm  1-166.dcm  1-196.dcm\n",
      "1-017.dcm  1-047.dcm  1-077.dcm  1-107.dcm  1-137.dcm  1-167.dcm  1-197.dcm\n",
      "1-018.dcm  1-048.dcm  1-078.dcm  1-108.dcm  1-138.dcm  1-168.dcm  1-198.dcm\n",
      "1-019.dcm  1-049.dcm  1-079.dcm  1-109.dcm  1-139.dcm  1-169.dcm  1-199.dcm\n",
      "1-020.dcm  1-050.dcm  1-080.dcm  1-110.dcm  1-140.dcm  1-170.dcm  1-200.dcm\n",
      "1-021.dcm  1-051.dcm  1-081.dcm  1-111.dcm  1-141.dcm  1-171.dcm  1-201.dcm\n",
      "1-022.dcm  1-052.dcm  1-082.dcm  1-112.dcm  1-142.dcm  1-172.dcm  1-202.dcm\n",
      "1-023.dcm  1-053.dcm  1-083.dcm  1-113.dcm  1-143.dcm  1-173.dcm  1-203.dcm\n",
      "1-024.dcm  1-054.dcm  1-084.dcm  1-114.dcm  1-144.dcm  1-174.dcm  1-204.dcm\n",
      "1-025.dcm  1-055.dcm  1-085.dcm  1-115.dcm  1-145.dcm  1-175.dcm\n",
      "1-026.dcm  1-056.dcm  1-086.dcm  1-116.dcm  1-146.dcm  1-176.dcm\n",
      "1-027.dcm  1-057.dcm  1-087.dcm  1-117.dcm  1-147.dcm  1-177.dcm\n",
      "1-028.dcm  1-058.dcm  1-088.dcm  1-118.dcm  1-148.dcm  1-178.dcm\n",
      "1-029.dcm  1-059.dcm  1-089.dcm  1-119.dcm  1-149.dcm  1-179.dcm\n",
      "1-030.dcm  1-060.dcm  1-090.dcm  1-120.dcm  1-150.dcm  1-180.dcm\n"
     ]
    }
   ],
   "source": [
    "%env HOLOSCAN_INPUT_PATH dcm\n",
    "%env HOLOSCAN_MODEL_PATH multi_models\n",
    "%env HOLOSCAN_OUTPUT_PATH output\n",
    "%ls $HOLOSCAN_INPUT_PATH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up imports\n",
    "\n",
    "Let's import necessary classes/decorators to define Application and Operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Required for setting SegmentDescription attributes. Direct import as this is not part of App SDK package.\n",
    "from pydicom.sr.codedict import codes\n",
    "\n",
    "from monai.deploy.conditions import CountCondition\n",
    "from monai.deploy.core import AppContext, Application\n",
    "from monai.deploy.core.domain import Image\n",
    "from monai.deploy.core.io_type import IOType\n",
    "from monai.deploy.operators.dicom_data_loader_operator import DICOMDataLoaderOperator\n",
    "from monai.deploy.operators.dicom_seg_writer_operator import DICOMSegmentationWriterOperator, SegmentDescription\n",
    "from monai.deploy.operators.dicom_series_selector_operator import DICOMSeriesSelectorOperator\n",
    "from monai.deploy.operators.dicom_series_to_volume_operator import DICOMSeriesToVolumeOperator\n",
    "from monai.deploy.operators.monai_bundle_inference_operator import (\n",
    "    BundleConfigNames,\n",
    "    IOMapping,\n",
    "    MonaiBundleInferenceOperator,\n",
    ")\n",
    "from monai.deploy.operators.stl_conversion_operator import STLConversionOperator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the Model Name and I/O for the Model Bundle Inference Operator\n",
    "\n",
    "The App SDK provides a `MonaiBundleInferenceOperator` class to perform inference with a MONAI Bundle, which is essentially a PyTorch model in TorchScript with additional metadata describing the model network and processing specification. This operator uses the MONAI utilities to parse a MONAI Bundle to automatically instantiate the objects required for input and output processing as well as inference, as such it depends on MONAI transforms, inferers, and in turn their dependencies.\n",
    "\n",
    "Each Operator class inherits from the base `Operator` class, and its named input/output properties are specified by using the function `setup`. For the `MonaiBundleInferenceOperator` class, the input/output need to be defined to match those of the model network, both in name and data type. For the current release, an `IOMapping` object is used to connect the operator input/output to those of the model network by matching the names. This is subject to change in the future releases.\n",
    "\n",
    "When multiple models are used in an application, it is important that each model network name is passed in when creating the inference operator, via the `model_name` argument.\n",
    "\n",
    "Both the Spleen CT Segmentation and Pancreas model networks have a named input, called \"image\", and a named output called \"pred\", both are of image type. These can all be mapped to the App SDK [Image](/modules/_autosummary/monai.deploy.core.domain.Image). The information on model network input and output data types is typically acquired by examining the model metadata `network_data_format` in the MONAI Bundle, as seen in this [example] (https://github.com/Project-MONAI/model-zoo/blob/dev/models/spleen_ct_segmentation/configs/metadata.json)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Application class\n",
    "\n",
    "Our application class would look like below.\n",
    "\n",
    "It defines `App` class, inheriting `Application` class.\n",
    "\n",
    "Objects required for DICOM parsing, series selection, pixel data conversion to volume image, model specific inference, and the AI result specific DICOM Segmentation object writers are created. The execution pipeline, as a Directed Acyclic Graph, is then constructed by connecting these objects through function `add_flow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class App(Application):\n",
    "    \"\"\"This example demonstrates how to create a multi-model/multi-AI application.\n",
    "\n",
    "    The important steps are:\n",
    "        1. Place the model TorchScripts in a defined folder structure, see below for details\n",
    "        2. Pass the model name to the inference operator instance in the app\n",
    "        3. Connect the input to and output from the inference operators, as required by the app\n",
    "\n",
    "    Required Model Folder Structure:\n",
    "        1. The model TorchScripts, be it MONAI Bundle compliant or not, must be placed in\n",
    "           a parent folder, whose path is used as the path to the model(s) on app execution\n",
    "        2. Each TorchScript file needs to be in a sub-folder, whose name is the model name\n",
    "\n",
    "    An example is shown below, where the `parent_foler` name can be the app's own choosing, and\n",
    "    the sub-folder names become model names, `pancreas_ct_dints` and `spleen_model`, respectively.\n",
    "\n",
    "        <parent_fodler>\n",
    "        ├── pancreas_ct_dints\n",
    "        │   └── model.ts\n",
    "        └── spleen_ct\n",
    "            └── model.ts\n",
    "\n",
    "    Note:\n",
    "    1. The TorchScript files of MONAI Bundles can be downloaded from MONAI Model Zoo, at\n",
    "       https://github.com/Project-MONAI/model-zoo/tree/dev/models\n",
    "       https://github.com/Project-MONAI/model-zoo/tree/dev/models/spleen_ct_segmentation, v0.3.2\n",
    "       https://github.com/Project-MONAI/model-zoo/tree/dev/models/pancreas_ct_dints_segmentation, v0.3.8\n",
    "    2. The input DICOM instances are from a DICOM Series of CT Abdomen, similar to the ones\n",
    "       used in the Spleen Segmentation example\n",
    "    3. This example is purely for technical demonstration, not for clinical use\n",
    "\n",
    "    Execution Time Estimate:\n",
    "      With a Nvidia GV100 32GB GPU, the execution time is around 87 seconds for an input DICOM series of 204 instances,\n",
    "      and 167 second for a series of 515 instances.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Creates an application instance.\"\"\"\n",
    "        self._logger = logging.getLogger(\"{}.{}\".format(__name__, type(self).__name__))\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def run(self, *args, **kwargs):\n",
    "        # This method calls the base class to run. Can be omitted if simply calling through.\n",
    "        self._logger.info(f\"Begin {self.run.__name__}\")\n",
    "        super().run(*args, **kwargs)\n",
    "        self._logger.info(f\"End {self.run.__name__}\")\n",
    "\n",
    "    def compose(self):\n",
    "        \"\"\"Creates the app specific operators and chain them up in the processing DAG.\"\"\"\n",
    "\n",
    "        logging.info(f\"Begin {self.compose.__name__}\")\n",
    "\n",
    "        app_context = AppContext({})  # Let it figure out all the attributes without overriding\n",
    "        app_input_path = Path(app_context.input_path)\n",
    "        app_output_path = Path(app_context.output_path)\n",
    "\n",
    "        # Create the custom operator(s) as well as SDK built-in operator(s).\n",
    "        study_loader_op = DICOMDataLoaderOperator(\n",
    "            self, CountCondition(self, 1), input_folder=app_input_path, name=\"study_loader_op\"\n",
    "        )\n",
    "        series_selector_op = DICOMSeriesSelectorOperator(self, rules=Sample_Rules_Text, name=\"series_selector_op\")\n",
    "        series_to_vol_op = DICOMSeriesToVolumeOperator(self, name=\"series_to_vol_op\")\n",
    "\n",
    "        # Create the inference operator that supports MONAI Bundle and automates the inference.\n",
    "        # The IOMapping labels match the input and prediction keys in the pre and post processing.\n",
    "        # The model_name needs to be provided as this is a multi-model application and each inference\n",
    "        # operator need to rely on the name to access the named loaded model network.\n",
    "        # create an inference operator for each.\n",
    "        #\n",
    "        # Pertinent MONAI Bundle:\n",
    "        #   https://github.com/Project-MONAI/model-zoo/tree/dev/models/spleen_ct_segmentation, v0.3.2\n",
    "        #   https://github.com/Project-MONAI/model-zoo/tree/dev/models/pancreas_ct_dints_segmentation, v0.3.8\n",
    "\n",
    "        config_names = BundleConfigNames(config_names=[\"inference\"])  # Same as the default\n",
    "\n",
    "        # This is the inference operator for the spleen_model bundle. Note the model name.\n",
    "        bundle_spleen_seg_op = MonaiBundleInferenceOperator(\n",
    "            self,\n",
    "            input_mapping=[IOMapping(\"image\", Image, IOType.IN_MEMORY)],\n",
    "            output_mapping=[IOMapping(\"pred\", Image, IOType.IN_MEMORY)],\n",
    "            app_context=app_context,\n",
    "            bundle_config_names=config_names,\n",
    "            model_name=\"spleen_ct\",\n",
    "            name=\"bundle_spleen_seg_op\",\n",
    "        )\n",
    "\n",
    "        # This is the inference operator for the pancreas_ct_dints bundle. Note the model name.\n",
    "        bundle_pancreas_seg_op = MonaiBundleInferenceOperator(\n",
    "            self,\n",
    "            input_mapping=[IOMapping(\"image\", Image, IOType.IN_MEMORY)],\n",
    "            output_mapping=[IOMapping(\"pred\", Image, IOType.IN_MEMORY)],\n",
    "            app_context=app_context,\n",
    "            bundle_config_names=config_names,\n",
    "            model_name=\"pancreas_ct_dints\",\n",
    "            name=\"bundle_pancreas_seg_op\",\n",
    "        )\n",
    "\n",
    "        # Create DICOM Seg writer providing the required segment description for each segment with\n",
    "        # the actual algorithm and the pertinent organ/tissue. The segment_label, algorithm_name,\n",
    "        # and algorithm_version are of DICOM VR LO type, limited to 64 chars.\n",
    "        # https://dicom.nema.org/medical/dicom/current/output/chtml/part05/sect_6.2.html\n",
    "        #\n",
    "        # NOTE: Each generated DICOM Seg will be a dcm file with the name based on the SOP instance UID.\n",
    "\n",
    "        # Description for the Spleen seg, and the seg writer obj\n",
    "        seg_descriptions_spleen = [\n",
    "            SegmentDescription(\n",
    "                segment_label=\"Spleen\",\n",
    "                segmented_property_category=codes.SCT.Organ,\n",
    "                segmented_property_type=codes.SCT.Spleen,\n",
    "                algorithm_name=\"volumetric (3D) segmentation of the spleen from CT image\",\n",
    "                algorithm_family=codes.DCM.ArtificialIntelligence,\n",
    "                algorithm_version=\"0.3.2\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        custom_tags_spleen = {\"SeriesDescription\": \"AI Spleen Seg for research use only. Not for clinical use.\"}\n",
    "        dicom_seg_writer_spleen = DICOMSegmentationWriterOperator(\n",
    "            self,\n",
    "            segment_descriptions=seg_descriptions_spleen,\n",
    "            custom_tags=custom_tags_spleen,\n",
    "            output_folder=app_output_path,\n",
    "            name=\"dicom_seg_writer_spleen\",\n",
    "        )\n",
    "\n",
    "        # Description for the Pancreas seg, and the seg writer obj\n",
    "        _algorithm_name = \"Pancreas CT DiNTS segmentation from CT image\"\n",
    "        _algorithm_family = codes.DCM.ArtificialIntelligence\n",
    "        _algorithm_version = \"0.3.8\"\n",
    "\n",
    "        seg_descriptions_pancreas = [\n",
    "            SegmentDescription(\n",
    "                segment_label=\"Pancreas\",\n",
    "                segmented_property_category=codes.SCT.Organ,\n",
    "                segmented_property_type=codes.SCT.Pancreas,\n",
    "                algorithm_name=_algorithm_name,\n",
    "                algorithm_family=_algorithm_family,\n",
    "                algorithm_version=_algorithm_version,\n",
    "            ),\n",
    "            SegmentDescription(\n",
    "                segment_label=\"Tumor\",\n",
    "                segmented_property_category=codes.SCT.Tumor,\n",
    "                segmented_property_type=codes.SCT.Tumor,\n",
    "                algorithm_name=_algorithm_name,\n",
    "                algorithm_family=_algorithm_family,\n",
    "                algorithm_version=_algorithm_version,\n",
    "            ),\n",
    "        ]\n",
    "        custom_tags_pancreas = {\"SeriesDescription\": \"AI Pancreas Seg for research use only. Not for clinical use.\"}\n",
    "\n",
    "        dicom_seg_writer_pancreas = DICOMSegmentationWriterOperator(\n",
    "            self,\n",
    "            segment_descriptions=seg_descriptions_pancreas,\n",
    "            custom_tags=custom_tags_pancreas,\n",
    "            output_folder=app_output_path,\n",
    "            name=\"dicom_seg_writer_pancreas\",\n",
    "        )\n",
    "\n",
    "        # NOTE: Sharp eyed readers can already see that the above instantiation of object can be simply parameterized.\n",
    "        #       Very true, but leaving them as if for easy reading. In fact the whole app can be parameterized for general use.\n",
    "\n",
    "        # Create the processing pipeline, by specifying the upstream and downstream operators, and\n",
    "        # ensuring the output from the former matches the input of the latter, in both name and type.\n",
    "        self.add_flow(study_loader_op, series_selector_op, {(\"dicom_study_list\", \"dicom_study_list\")})\n",
    "        self.add_flow(\n",
    "            series_selector_op, series_to_vol_op, {(\"study_selected_series_list\", \"study_selected_series_list\")}\n",
    "        )\n",
    "\n",
    "        # Feed the input image to all inference operators\n",
    "        self.add_flow(series_to_vol_op, bundle_spleen_seg_op, {(\"image\", \"image\")})\n",
    "        # The Pancreas CT Seg bundle requires PyTorch 1.12.0 to avoid failure to load.\n",
    "        self.add_flow(series_to_vol_op, bundle_pancreas_seg_op, {(\"image\", \"image\")})\n",
    "\n",
    "        # Create DICOM Seg for one of the inference output\n",
    "        # Note below the dicom_seg_writer requires two inputs, each coming from a upstream operator.\n",
    "        self.add_flow(\n",
    "            series_selector_op, dicom_seg_writer_spleen, {(\"study_selected_series_list\", \"study_selected_series_list\")}\n",
    "        )\n",
    "        self.add_flow(bundle_spleen_seg_op, dicom_seg_writer_spleen, {(\"pred\", \"seg_image\")})\n",
    "\n",
    "        # Create DICOM Seg for one of the inference output\n",
    "        # Note below the dicom_seg_writer requires two inputs, each coming from a upstream operator.\n",
    "        self.add_flow(\n",
    "            series_selector_op,\n",
    "            dicom_seg_writer_pancreas,\n",
    "            {(\"study_selected_series_list\", \"study_selected_series_list\")},\n",
    "        )\n",
    "        self.add_flow(bundle_pancreas_seg_op, dicom_seg_writer_pancreas, {(\"pred\", \"seg_image\")})\n",
    "\n",
    "        logging.info(f\"End {self.compose.__name__}\")\n",
    "\n",
    "\n",
    "# This is a sample series selection rule in JSON, simply selecting CT series.\n",
    "# If the study has more than 1 CT series, then all of them will be selected.\n",
    "# Please see more detail in DICOMSeriesSelectorOperator.\n",
    "Sample_Rules_Text = \"\"\"\n",
    "{\n",
    "    \"selections\": [\n",
    "        {\n",
    "            \"name\": \"CT Series\",\n",
    "            \"conditions\": {\n",
    "                \"StudyDescription\": \"(.*?)\",\n",
    "                \"Modality\": \"(?i)CT\",\n",
    "                \"SeriesDescription\": \"(.*?)\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing app locally\n",
    "\n",
    "We can execute the app in the Jupyter notebook. Note that the DICOM files of the CT Abdomen series must be present in the input folder, the models are already staged, and and environment variables are set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-18 20:02:39,359 - Begin run\n",
      "2023-07-18 20:02:39,360 - Begin compose\n",
      "2023-07-18 20:02:39,372 - End compose\n",
      "2023-07-18 20:02:39,445 - No or invalid input path from the optional input port: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[info] [gxf_executor.cpp:182] Creating context\n",
      "[info] [gxf_executor.cpp:1576] Loading extensions from configs...\n",
      "[info] [gxf_executor.cpp:1718] Activating Graph...\n",
      "[info] [gxf_executor.cpp:1748] Running Graph...\n",
      "[info] [gxf_executor.cpp:1750] Waiting for completion...\n",
      "[info] [gxf_executor.cpp:1751] Graph execution waiting. Fragment: \n",
      "[info] [greedy_scheduler.cpp:190] Scheduling 9 entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-18 20:02:39,996 - Finding series for Selection named: CT Series\n",
      "2023-07-18 20:02:39,997 - Searching study, : 1.3.6.1.4.1.14519.5.2.1.7085.2626.822645453932810382886582736291\n",
      "  # of series: 1\n",
      "2023-07-18 20:02:39,998 - Working on series, instance UID: 1.3.6.1.4.1.14519.5.2.1.7085.2626.119403521930927333027265674239\n",
      "2023-07-18 20:02:39,999 - On attribute: 'StudyDescription' to match value: '(.*?)'\n",
      "2023-07-18 20:02:39,999 -     Series attribute StudyDescription value: CT ABDOMEN W IV CONTRAST\n",
      "2023-07-18 20:02:40,000 - Series attribute string value did not match. Try regEx.\n",
      "2023-07-18 20:02:40,001 - On attribute: 'Modality' to match value: '(?i)CT'\n",
      "2023-07-18 20:02:40,002 -     Series attribute Modality value: CT\n",
      "2023-07-18 20:02:40,002 - Series attribute string value did not match. Try regEx.\n",
      "2023-07-18 20:02:40,003 - On attribute: 'SeriesDescription' to match value: '(.*?)'\n",
      "2023-07-18 20:02:40,003 -     Series attribute SeriesDescription value: ABD/PANC 3.0 B31f\n",
      "2023-07-18 20:02:40,004 - Series attribute string value did not match. Try regEx.\n",
      "2023-07-18 20:02:40,004 - Selected Series, UID: 1.3.6.1.4.1.14519.5.2.1.7085.2626.119403521930927333027265674239\n",
      "2023-07-18 20:02:40,213 - Parsing from bundle_path: /home/mqin/src/monai-deploy-app-sdk/notebooks/tutorials/multi_models/pancreas_ct_dints/model.ts\n",
      "2023-07-18 20:04:13,588 - Parsing from bundle_path: /home/mqin/src/monai-deploy-app-sdk/notebooks/tutorials/multi_models/spleen_ct/model.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages/highdicom/valuerep.py:54: UserWarning: The string \"C3N-00198\" is unlikely to represent the intended person name since it contains only a single component. Construct a person name according to the format in described in http://dicom.nema.org/dicom/2013/output/chtml/part05/sect_6.2.html#sect_6.2.1.2, or, in pydicom 2.2.0 or later, use the pydicom.valuerep.PersonName.from_named_components() method to construct the person name correctly. If a single-component name is really intended, add a trailing caret character to disambiguate the name.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-18 20:04:19,629 - add plane #0 for segment #1\n",
      "2023-07-18 20:04:19,631 - add plane #1 for segment #1\n",
      "2023-07-18 20:04:19,633 - add plane #2 for segment #1\n",
      "2023-07-18 20:04:19,635 - add plane #3 for segment #1\n",
      "2023-07-18 20:04:19,636 - add plane #4 for segment #1\n",
      "2023-07-18 20:04:19,638 - add plane #5 for segment #1\n",
      "2023-07-18 20:04:19,639 - add plane #6 for segment #1\n",
      "2023-07-18 20:04:19,641 - add plane #7 for segment #1\n",
      "2023-07-18 20:04:19,643 - add plane #8 for segment #1\n",
      "2023-07-18 20:04:19,644 - add plane #9 for segment #1\n",
      "2023-07-18 20:04:19,646 - add plane #10 for segment #1\n",
      "2023-07-18 20:04:19,649 - add plane #11 for segment #1\n",
      "2023-07-18 20:04:19,651 - add plane #12 for segment #1\n",
      "2023-07-18 20:04:19,653 - add plane #13 for segment #1\n",
      "2023-07-18 20:04:19,654 - add plane #14 for segment #1\n",
      "2023-07-18 20:04:19,656 - add plane #15 for segment #1\n",
      "2023-07-18 20:04:19,657 - add plane #16 for segment #1\n",
      "2023-07-18 20:04:19,659 - add plane #17 for segment #1\n",
      "2023-07-18 20:04:19,661 - add plane #18 for segment #1\n",
      "2023-07-18 20:04:19,662 - add plane #19 for segment #1\n",
      "2023-07-18 20:04:19,664 - add plane #20 for segment #1\n",
      "2023-07-18 20:04:19,666 - add plane #21 for segment #1\n",
      "2023-07-18 20:04:19,667 - add plane #22 for segment #1\n",
      "2023-07-18 20:04:19,669 - add plane #23 for segment #1\n",
      "2023-07-18 20:04:19,671 - add plane #24 for segment #1\n",
      "2023-07-18 20:04:19,672 - add plane #25 for segment #1\n",
      "2023-07-18 20:04:19,674 - add plane #26 for segment #1\n",
      "2023-07-18 20:04:19,676 - add plane #27 for segment #1\n",
      "2023-07-18 20:04:19,678 - add plane #28 for segment #1\n",
      "2023-07-18 20:04:19,680 - add plane #29 for segment #1\n",
      "2023-07-18 20:04:19,682 - add plane #30 for segment #1\n",
      "2023-07-18 20:04:19,684 - add plane #31 for segment #1\n",
      "2023-07-18 20:04:19,686 - add plane #32 for segment #1\n",
      "2023-07-18 20:04:19,688 - add plane #33 for segment #1\n",
      "2023-07-18 20:04:19,690 - add plane #34 for segment #1\n",
      "2023-07-18 20:04:19,692 - add plane #35 for segment #1\n",
      "2023-07-18 20:04:19,694 - add plane #36 for segment #1\n",
      "2023-07-18 20:04:19,696 - add plane #37 for segment #1\n",
      "2023-07-18 20:04:19,699 - add plane #38 for segment #1\n",
      "2023-07-18 20:04:19,702 - add plane #39 for segment #1\n",
      "2023-07-18 20:04:19,704 - add plane #40 for segment #1\n",
      "2023-07-18 20:04:19,707 - add plane #41 for segment #1\n",
      "2023-07-18 20:04:19,709 - add plane #42 for segment #1\n",
      "2023-07-18 20:04:19,712 - add plane #43 for segment #1\n",
      "2023-07-18 20:04:19,715 - add plane #44 for segment #1\n",
      "2023-07-18 20:04:19,717 - add plane #45 for segment #1\n",
      "2023-07-18 20:04:19,720 - add plane #46 for segment #1\n",
      "2023-07-18 20:04:19,722 - add plane #47 for segment #1\n",
      "2023-07-18 20:04:19,725 - add plane #48 for segment #1\n",
      "2023-07-18 20:04:19,727 - add plane #49 for segment #1\n",
      "2023-07-18 20:04:19,730 - add plane #50 for segment #1\n",
      "2023-07-18 20:04:19,734 - add plane #51 for segment #1\n",
      "2023-07-18 20:04:19,736 - add plane #52 for segment #1\n",
      "2023-07-18 20:04:19,738 - add plane #53 for segment #1\n",
      "2023-07-18 20:04:19,741 - add plane #54 for segment #1\n",
      "2023-07-18 20:04:19,743 - add plane #55 for segment #1\n",
      "2023-07-18 20:04:19,745 - add plane #56 for segment #1\n",
      "2023-07-18 20:04:19,748 - add plane #57 for segment #1\n",
      "2023-07-18 20:04:19,751 - add plane #58 for segment #1\n",
      "2023-07-18 20:04:19,753 - add plane #59 for segment #1\n",
      "2023-07-18 20:04:19,755 - add plane #60 for segment #1\n",
      "2023-07-18 20:04:19,757 - add plane #61 for segment #1\n",
      "2023-07-18 20:04:19,760 - add plane #62 for segment #1\n",
      "2023-07-18 20:04:19,762 - add plane #63 for segment #1\n",
      "2023-07-18 20:04:19,764 - add plane #64 for segment #1\n",
      "2023-07-18 20:04:19,766 - add plane #65 for segment #1\n",
      "2023-07-18 20:04:19,768 - add plane #66 for segment #1\n",
      "2023-07-18 20:04:19,770 - add plane #67 for segment #1\n",
      "2023-07-18 20:04:19,792 - skip empty plane 0 of segment #2\n",
      "2023-07-18 20:04:19,793 - skip empty plane 1 of segment #2\n",
      "2023-07-18 20:04:19,793 - skip empty plane 2 of segment #2\n",
      "2023-07-18 20:04:19,794 - skip empty plane 3 of segment #2\n",
      "2023-07-18 20:04:19,795 - skip empty plane 4 of segment #2\n",
      "2023-07-18 20:04:19,796 - skip empty plane 5 of segment #2\n",
      "2023-07-18 20:04:19,796 - skip empty plane 6 of segment #2\n",
      "2023-07-18 20:04:19,797 - skip empty plane 7 of segment #2\n",
      "2023-07-18 20:04:19,798 - skip empty plane 8 of segment #2\n",
      "2023-07-18 20:04:19,798 - skip empty plane 9 of segment #2\n",
      "2023-07-18 20:04:19,799 - skip empty plane 10 of segment #2\n",
      "2023-07-18 20:04:19,799 - skip empty plane 11 of segment #2\n",
      "2023-07-18 20:04:19,800 - skip empty plane 12 of segment #2\n",
      "2023-07-18 20:04:19,801 - skip empty plane 13 of segment #2\n",
      "2023-07-18 20:04:19,801 - skip empty plane 14 of segment #2\n",
      "2023-07-18 20:04:19,802 - skip empty plane 15 of segment #2\n",
      "2023-07-18 20:04:19,803 - skip empty plane 16 of segment #2\n",
      "2023-07-18 20:04:19,804 - skip empty plane 17 of segment #2\n",
      "2023-07-18 20:04:19,804 - skip empty plane 18 of segment #2\n",
      "2023-07-18 20:04:19,806 - skip empty plane 19 of segment #2\n",
      "2023-07-18 20:04:19,806 - skip empty plane 20 of segment #2\n",
      "2023-07-18 20:04:19,807 - skip empty plane 21 of segment #2\n",
      "2023-07-18 20:04:19,808 - skip empty plane 22 of segment #2\n",
      "2023-07-18 20:04:19,808 - skip empty plane 23 of segment #2\n",
      "2023-07-18 20:04:19,810 - skip empty plane 24 of segment #2\n",
      "2023-07-18 20:04:19,811 - skip empty plane 25 of segment #2\n",
      "2023-07-18 20:04:19,811 - skip empty plane 26 of segment #2\n",
      "2023-07-18 20:04:19,812 - skip empty plane 27 of segment #2\n",
      "2023-07-18 20:04:19,813 - skip empty plane 28 of segment #2\n",
      "2023-07-18 20:04:19,814 - skip empty plane 29 of segment #2\n",
      "2023-07-18 20:04:19,814 - skip empty plane 30 of segment #2\n",
      "2023-07-18 20:04:19,815 - skip empty plane 31 of segment #2\n",
      "2023-07-18 20:04:19,816 - skip empty plane 32 of segment #2\n",
      "2023-07-18 20:04:19,816 - skip empty plane 33 of segment #2\n",
      "2023-07-18 20:04:19,817 - skip empty plane 34 of segment #2\n",
      "2023-07-18 20:04:19,818 - skip empty plane 35 of segment #2\n",
      "2023-07-18 20:04:19,818 - skip empty plane 36 of segment #2\n",
      "2023-07-18 20:04:19,819 - skip empty plane 37 of segment #2\n",
      "2023-07-18 20:04:19,820 - skip empty plane 38 of segment #2\n",
      "2023-07-18 20:04:19,820 - skip empty plane 39 of segment #2\n",
      "2023-07-18 20:04:19,821 - skip empty plane 40 of segment #2\n",
      "2023-07-18 20:04:19,822 - skip empty plane 41 of segment #2\n",
      "2023-07-18 20:04:19,822 - skip empty plane 42 of segment #2\n",
      "2023-07-18 20:04:19,823 - skip empty plane 43 of segment #2\n",
      "2023-07-18 20:04:19,824 - skip empty plane 44 of segment #2\n",
      "2023-07-18 20:04:19,825 - skip empty plane 45 of segment #2\n",
      "2023-07-18 20:04:19,825 - skip empty plane 46 of segment #2\n",
      "2023-07-18 20:04:19,826 - skip empty plane 47 of segment #2\n",
      "2023-07-18 20:04:19,827 - skip empty plane 48 of segment #2\n",
      "2023-07-18 20:04:19,827 - skip empty plane 49 of segment #2\n",
      "2023-07-18 20:04:19,828 - skip empty plane 50 of segment #2\n",
      "2023-07-18 20:04:19,829 - skip empty plane 51 of segment #2\n",
      "2023-07-18 20:04:19,830 - skip empty plane 52 of segment #2\n",
      "2023-07-18 20:04:19,830 - skip empty plane 53 of segment #2\n",
      "2023-07-18 20:04:19,831 - skip empty plane 54 of segment #2\n",
      "2023-07-18 20:04:19,832 - skip empty plane 55 of segment #2\n",
      "2023-07-18 20:04:19,833 - skip empty plane 56 of segment #2\n",
      "2023-07-18 20:04:19,834 - skip empty plane 57 of segment #2\n",
      "2023-07-18 20:04:19,835 - skip empty plane 58 of segment #2\n",
      "2023-07-18 20:04:19,836 - skip empty plane 59 of segment #2\n",
      "2023-07-18 20:04:19,836 - skip empty plane 60 of segment #2\n",
      "2023-07-18 20:04:19,838 - skip empty plane 61 of segment #2\n",
      "2023-07-18 20:04:19,839 - skip empty plane 62 of segment #2\n",
      "2023-07-18 20:04:19,841 - skip empty plane 63 of segment #2\n",
      "2023-07-18 20:04:19,842 - skip empty plane 64 of segment #2\n",
      "2023-07-18 20:04:19,843 - skip empty plane 65 of segment #2\n",
      "2023-07-18 20:04:19,844 - skip empty plane 66 of segment #2\n",
      "2023-07-18 20:04:19,846 - skip empty plane 67 of segment #2\n",
      "2023-07-18 20:04:19,881 - copy Image-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:04:19,882 - copy attributes of module \"Specimen\"\n",
      "2023-07-18 20:04:19,883 - copy Patient-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:04:19,883 - copy attributes of module \"Patient\"\n",
      "2023-07-18 20:04:19,884 - copy attributes of module \"Clinical Trial Subject\"\n",
      "2023-07-18 20:04:19,885 - copy Study-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:04:19,885 - copy attributes of module \"General Study\"\n",
      "2023-07-18 20:04:19,886 - copy attributes of module \"Patient Study\"\n",
      "2023-07-18 20:04:19,887 - copy attributes of module \"Clinical Trial Study\"\n",
      "2023-07-18 20:04:21,262 - add plane #0 for segment #1\n",
      "2023-07-18 20:04:21,264 - add plane #1 for segment #1\n",
      "2023-07-18 20:04:21,266 - add plane #2 for segment #1\n",
      "2023-07-18 20:04:21,268 - add plane #3 for segment #1\n",
      "2023-07-18 20:04:21,269 - add plane #4 for segment #1\n",
      "2023-07-18 20:04:21,271 - add plane #5 for segment #1\n",
      "2023-07-18 20:04:21,273 - add plane #6 for segment #1\n",
      "2023-07-18 20:04:21,274 - add plane #7 for segment #1\n",
      "2023-07-18 20:04:21,276 - add plane #8 for segment #1\n",
      "2023-07-18 20:04:21,277 - add plane #9 for segment #1\n",
      "2023-07-18 20:04:21,279 - add plane #10 for segment #1\n",
      "2023-07-18 20:04:21,281 - add plane #11 for segment #1\n",
      "2023-07-18 20:04:21,282 - add plane #12 for segment #1\n",
      "2023-07-18 20:04:21,284 - add plane #13 for segment #1\n",
      "2023-07-18 20:04:21,285 - add plane #14 for segment #1\n",
      "2023-07-18 20:04:21,287 - add plane #15 for segment #1\n",
      "2023-07-18 20:04:21,288 - add plane #16 for segment #1\n",
      "2023-07-18 20:04:21,290 - add plane #17 for segment #1\n",
      "2023-07-18 20:04:21,291 - add plane #18 for segment #1\n",
      "2023-07-18 20:04:21,293 - add plane #19 for segment #1\n",
      "2023-07-18 20:04:21,295 - add plane #20 for segment #1\n",
      "2023-07-18 20:04:21,296 - add plane #21 for segment #1\n",
      "2023-07-18 20:04:21,298 - add plane #22 for segment #1\n",
      "2023-07-18 20:04:21,299 - add plane #23 for segment #1\n",
      "2023-07-18 20:04:21,301 - add plane #24 for segment #1\n",
      "2023-07-18 20:04:21,303 - add plane #25 for segment #1\n",
      "2023-07-18 20:04:21,304 - add plane #26 for segment #1\n",
      "2023-07-18 20:04:21,306 - add plane #27 for segment #1\n",
      "2023-07-18 20:04:21,308 - add plane #28 for segment #1\n",
      "2023-07-18 20:04:21,310 - add plane #29 for segment #1\n",
      "2023-07-18 20:04:21,312 - add plane #30 for segment #1\n",
      "2023-07-18 20:04:21,314 - add plane #31 for segment #1\n",
      "2023-07-18 20:04:21,316 - add plane #32 for segment #1\n",
      "2023-07-18 20:04:21,318 - add plane #33 for segment #1\n",
      "2023-07-18 20:04:21,320 - add plane #34 for segment #1\n",
      "2023-07-18 20:04:21,322 - add plane #35 for segment #1\n",
      "2023-07-18 20:04:21,324 - add plane #36 for segment #1\n",
      "2023-07-18 20:04:21,326 - add plane #37 for segment #1\n",
      "2023-07-18 20:04:21,328 - add plane #38 for segment #1\n",
      "2023-07-18 20:04:21,330 - add plane #39 for segment #1\n",
      "2023-07-18 20:04:21,332 - add plane #40 for segment #1\n",
      "2023-07-18 20:04:21,334 - add plane #41 for segment #1\n",
      "2023-07-18 20:04:21,335 - add plane #42 for segment #1\n",
      "2023-07-18 20:04:21,337 - add plane #43 for segment #1\n",
      "2023-07-18 20:04:21,339 - add plane #44 for segment #1\n",
      "2023-07-18 20:04:21,341 - add plane #45 for segment #1\n",
      "2023-07-18 20:04:21,343 - add plane #46 for segment #1\n",
      "2023-07-18 20:04:21,344 - add plane #47 for segment #1\n",
      "2023-07-18 20:04:21,346 - add plane #48 for segment #1\n",
      "2023-07-18 20:04:21,348 - add plane #49 for segment #1\n",
      "2023-07-18 20:04:21,349 - add plane #50 for segment #1\n",
      "2023-07-18 20:04:21,351 - add plane #51 for segment #1\n",
      "2023-07-18 20:04:21,353 - add plane #52 for segment #1\n",
      "2023-07-18 20:04:21,355 - add plane #53 for segment #1\n",
      "2023-07-18 20:04:21,357 - add plane #54 for segment #1\n",
      "2023-07-18 20:04:21,359 - add plane #55 for segment #1\n",
      "2023-07-18 20:04:21,360 - add plane #56 for segment #1\n",
      "2023-07-18 20:04:21,362 - add plane #57 for segment #1\n",
      "2023-07-18 20:04:21,364 - add plane #58 for segment #1\n",
      "2023-07-18 20:04:21,366 - add plane #59 for segment #1\n",
      "2023-07-18 20:04:21,369 - add plane #60 for segment #1\n",
      "2023-07-18 20:04:21,372 - add plane #61 for segment #1\n",
      "2023-07-18 20:04:21,374 - add plane #62 for segment #1\n",
      "2023-07-18 20:04:21,376 - add plane #63 for segment #1\n",
      "2023-07-18 20:04:21,378 - add plane #64 for segment #1\n",
      "2023-07-18 20:04:21,380 - add plane #65 for segment #1\n",
      "2023-07-18 20:04:21,381 - add plane #66 for segment #1\n",
      "2023-07-18 20:04:21,383 - add plane #67 for segment #1\n",
      "2023-07-18 20:04:21,386 - add plane #68 for segment #1\n",
      "2023-07-18 20:04:21,388 - add plane #69 for segment #1\n",
      "2023-07-18 20:04:21,390 - add plane #70 for segment #1\n",
      "2023-07-18 20:04:21,392 - add plane #71 for segment #1\n",
      "2023-07-18 20:04:21,394 - add plane #72 for segment #1\n",
      "2023-07-18 20:04:21,396 - add plane #73 for segment #1\n",
      "2023-07-18 20:04:21,398 - add plane #74 for segment #1\n",
      "2023-07-18 20:04:21,400 - add plane #75 for segment #1\n",
      "2023-07-18 20:04:21,403 - add plane #76 for segment #1\n",
      "2023-07-18 20:04:21,405 - add plane #77 for segment #1\n",
      "2023-07-18 20:04:21,407 - add plane #78 for segment #1\n",
      "2023-07-18 20:04:21,409 - add plane #79 for segment #1\n",
      "2023-07-18 20:04:21,411 - add plane #80 for segment #1\n",
      "2023-07-18 20:04:21,413 - add plane #81 for segment #1\n",
      "2023-07-18 20:04:21,415 - add plane #82 for segment #1\n",
      "2023-07-18 20:04:21,417 - add plane #83 for segment #1\n",
      "2023-07-18 20:04:21,419 - add plane #84 for segment #1\n",
      "2023-07-18 20:04:21,421 - add plane #85 for segment #1\n",
      "2023-07-18 20:04:21,423 - add plane #86 for segment #1\n",
      "2023-07-18 20:04:21,467 - copy Image-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:04:21,468 - copy attributes of module \"Specimen\"\n",
      "2023-07-18 20:04:21,469 - copy Patient-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:04:21,470 - copy attributes of module \"Patient\"\n",
      "2023-07-18 20:04:21,471 - copy attributes of module \"Clinical Trial Subject\"\n",
      "2023-07-18 20:04:21,471 - copy Study-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:04:21,472 - copy attributes of module \"General Study\"\n",
      "2023-07-18 20:04:21,473 - copy attributes of module \"Patient Study\"\n",
      "2023-07-18 20:04:21,474 - copy attributes of module \"Clinical Trial Study\"\n",
      "2023-07-18 20:04:21,598 - End run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[info] [greedy_scheduler.cpp:369] Scheduler stopped: Some entities are waiting for execution, but there are no periodic or async entities to get out of the deadlock.\n",
      "[info] [greedy_scheduler.cpp:398] Scheduler finished.\n",
      "[info] [gxf_executor.cpp:1760] Graph execution deactivating. Fragment: \n",
      "[info] [gxf_executor.cpp:1761] Deactivating Graph...\n",
      "[info] [gxf_executor.cpp:1764] Graph execution finished. Fragment: \n"
     ]
    }
   ],
   "source": [
    "app = App()\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the application is verified inside Jupyter notebook, we can write the whole application as a file, adding the following lines:\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    App().run()\n",
    "```\n",
    "\n",
    "The above lines are needed to execute the application code by using `python` interpreter.\n",
    "\n",
    "A `__main__.py` file should also be added, so the application folder structure would look like below:\n",
    "\n",
    "```bash\n",
    "my_app\n",
    "├── __main__.py\n",
    "└── app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an application folder\n",
    "!mkdir -p my_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_app/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_app/app.py\n",
    "# Copyright 2021-2023 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Required for setting SegmentDescription attributes. Direct import as this is not part of App SDK package.\n",
    "from pydicom.sr.codedict import codes\n",
    "\n",
    "from monai.deploy.conditions import CountCondition\n",
    "from monai.deploy.core import AppContext, Application\n",
    "from monai.deploy.core.domain import Image\n",
    "from monai.deploy.core.io_type import IOType\n",
    "from monai.deploy.operators.dicom_data_loader_operator import DICOMDataLoaderOperator\n",
    "from monai.deploy.operators.dicom_seg_writer_operator import DICOMSegmentationWriterOperator, SegmentDescription\n",
    "from monai.deploy.operators.dicom_series_selector_operator import DICOMSeriesSelectorOperator\n",
    "from monai.deploy.operators.dicom_series_to_volume_operator import DICOMSeriesToVolumeOperator\n",
    "from monai.deploy.operators.monai_bundle_inference_operator import (\n",
    "    BundleConfigNames,\n",
    "    IOMapping,\n",
    "    MonaiBundleInferenceOperator,\n",
    ")\n",
    "\n",
    "\n",
    "class App(Application):\n",
    "    \"\"\"This example demonstrates how to create a multi-model/multi-AI application.\n",
    "\n",
    "    The important steps are:\n",
    "        1. Place the model TorchScripts in a defined folder structure, see below for details\n",
    "        2. Pass the model name to the inference operator instance in the app\n",
    "        3. Connect the input to and output from the inference operators, as required by the app\n",
    "\n",
    "    Required Model Folder Structure:\n",
    "        1. The model TorchScripts, be it MONAI Bundle compliant or not, must be placed in\n",
    "           a parent folder, whose path is used as the path to the model(s) on app execution\n",
    "        2. Each TorchScript file needs to be in a sub-folder, whose name is the model name\n",
    "\n",
    "    An example is shown below, where the `parent_foler` name can be the app's own choosing, and\n",
    "    the sub-folder names become model names, `pancreas_ct_dints` and `spleen_model`, respectively.\n",
    "\n",
    "        <parent_fodler>\n",
    "        ├── pancreas_ct_dints\n",
    "        │   └── model.ts\n",
    "        └── spleen_ct\n",
    "            └── model.ts\n",
    "\n",
    "    Note:\n",
    "    1. The TorchScript files of MONAI Bundles can be downloaded from MONAI Model Zoo, at\n",
    "       https://github.com/Project-MONAI/model-zoo/tree/dev/models\n",
    "       https://github.com/Project-MONAI/model-zoo/tree/dev/models/spleen_ct_segmentation, v0.3.2\n",
    "       https://github.com/Project-MONAI/model-zoo/tree/dev/models/pancreas_ct_dints_segmentation, v0.3.8\n",
    "    2. The input DICOM instances are from a DICOM Series of CT Abdomen, similar to the ones\n",
    "       used in the Spleen Segmentation example\n",
    "    3. This example is purely for technical demonstration, not for clinical use\n",
    "\n",
    "    Execution Time Estimate:\n",
    "      With a Nvidia GV100 32GB GPU, the execution time is around 87 seconds for an input DICOM series of 204 instances,\n",
    "      and 167 second for a series of 515 instances.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Creates an application instance.\"\"\"\n",
    "        self._logger = logging.getLogger(\"{}.{}\".format(__name__, type(self).__name__))\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def run(self, *args, **kwargs):\n",
    "        # This method calls the base class to run. Can be omitted if simply calling through.\n",
    "        self._logger.info(f\"Begin {self.run.__name__}\")\n",
    "        super().run(*args, **kwargs)\n",
    "        self._logger.info(f\"End {self.run.__name__}\")\n",
    "\n",
    "    def compose(self):\n",
    "        \"\"\"Creates the app specific operators and chain them up in the processing DAG.\"\"\"\n",
    "\n",
    "        logging.info(f\"Begin {self.compose.__name__}\")\n",
    "\n",
    "        app_context = AppContext({})  # Let it figure out all the attributes without overriding\n",
    "        app_input_path = Path(app_context.input_path)\n",
    "        app_output_path = Path(app_context.output_path)\n",
    "\n",
    "        # Create the custom operator(s) as well as SDK built-in operator(s).\n",
    "        study_loader_op = DICOMDataLoaderOperator(\n",
    "            self, CountCondition(self, 1), input_folder=app_input_path, name=\"study_loader_op\"\n",
    "        )\n",
    "        series_selector_op = DICOMSeriesSelectorOperator(self, rules=Sample_Rules_Text, name=\"series_selector_op\")\n",
    "        series_to_vol_op = DICOMSeriesToVolumeOperator(self, name=\"series_to_vol_op\")\n",
    "\n",
    "        # Create the inference operator that supports MONAI Bundle and automates the inference.\n",
    "        # The IOMapping labels match the input and prediction keys in the pre and post processing.\n",
    "        # The model_name needs to be provided as this is a multi-model application and each inference\n",
    "        # operator need to rely on the name to access the named loaded model network.\n",
    "        # create an inference operator for each.\n",
    "        #\n",
    "        # Pertinent MONAI Bundle:\n",
    "        #   https://github.com/Project-MONAI/model-zoo/tree/dev/models/spleen_ct_segmentation, v0.3.2\n",
    "        #   https://github.com/Project-MONAI/model-zoo/tree/dev/models/pancreas_ct_dints_segmentation, v0.3.8\n",
    "\n",
    "        config_names = BundleConfigNames(config_names=[\"inference\"])  # Same as the default\n",
    "\n",
    "        # This is the inference operator for the spleen_model bundle. Note the model name.\n",
    "        bundle_spleen_seg_op = MonaiBundleInferenceOperator(\n",
    "            self,\n",
    "            input_mapping=[IOMapping(\"image\", Image, IOType.IN_MEMORY)],\n",
    "            output_mapping=[IOMapping(\"pred\", Image, IOType.IN_MEMORY)],\n",
    "            app_context=app_context,\n",
    "            bundle_config_names=config_names,\n",
    "            model_name=\"spleen_ct\",\n",
    "            name=\"bundle_spleen_seg_op\",\n",
    "        )\n",
    "\n",
    "        # This is the inference operator for the pancreas_ct_dints bundle. Note the model name.\n",
    "        bundle_pancreas_seg_op = MonaiBundleInferenceOperator(\n",
    "            self,\n",
    "            input_mapping=[IOMapping(\"image\", Image, IOType.IN_MEMORY)],\n",
    "            output_mapping=[IOMapping(\"pred\", Image, IOType.IN_MEMORY)],\n",
    "            app_context=app_context,\n",
    "            bundle_config_names=config_names,\n",
    "            model_name=\"pancreas_ct_dints\",\n",
    "            name=\"bundle_pancreas_seg_op\",\n",
    "        )\n",
    "\n",
    "        # Create DICOM Seg writer providing the required segment description for each segment with\n",
    "        # the actual algorithm and the pertinent organ/tissue. The segment_label, algorithm_name,\n",
    "        # and algorithm_version are of DICOM VR LO type, limited to 64 chars.\n",
    "        # https://dicom.nema.org/medical/dicom/current/output/chtml/part05/sect_6.2.html\n",
    "        #\n",
    "        # NOTE: Each generated DICOM Seg will be a dcm file with the name based on the SOP instance UID.\n",
    "\n",
    "        # Description for the Spleen seg, and the seg writer obj\n",
    "        seg_descriptions_spleen = [\n",
    "            SegmentDescription(\n",
    "                segment_label=\"Spleen\",\n",
    "                segmented_property_category=codes.SCT.Organ,\n",
    "                segmented_property_type=codes.SCT.Spleen,\n",
    "                algorithm_name=\"volumetric (3D) segmentation of the spleen from CT image\",\n",
    "                algorithm_family=codes.DCM.ArtificialIntelligence,\n",
    "                algorithm_version=\"0.3.2\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        custom_tags_spleen = {\"SeriesDescription\": \"AI Spleen Seg for research use only. Not for clinical use.\"}\n",
    "        dicom_seg_writer_spleen = DICOMSegmentationWriterOperator(\n",
    "            self,\n",
    "            segment_descriptions=seg_descriptions_spleen,\n",
    "            custom_tags=custom_tags_spleen,\n",
    "            output_folder=app_output_path,\n",
    "            name=\"dicom_seg_writer_spleen\",\n",
    "        )\n",
    "\n",
    "        # Description for the Pancreas seg, and the seg writer obj\n",
    "        _algorithm_name = \"Pancreas CT DiNTS segmentation from CT image\"\n",
    "        _algorithm_family = codes.DCM.ArtificialIntelligence\n",
    "        _algorithm_version = \"0.3.8\"\n",
    "\n",
    "        seg_descriptions_pancreas = [\n",
    "            SegmentDescription(\n",
    "                segment_label=\"Pancreas\",\n",
    "                segmented_property_category=codes.SCT.Organ,\n",
    "                segmented_property_type=codes.SCT.Pancreas,\n",
    "                algorithm_name=_algorithm_name,\n",
    "                algorithm_family=_algorithm_family,\n",
    "                algorithm_version=_algorithm_version,\n",
    "            ),\n",
    "            SegmentDescription(\n",
    "                segment_label=\"Tumor\",\n",
    "                segmented_property_category=codes.SCT.Tumor,\n",
    "                segmented_property_type=codes.SCT.Tumor,\n",
    "                algorithm_name=_algorithm_name,\n",
    "                algorithm_family=_algorithm_family,\n",
    "                algorithm_version=_algorithm_version,\n",
    "            ),\n",
    "        ]\n",
    "        custom_tags_pancreas = {\"SeriesDescription\": \"AI Pancreas Seg for research use only. Not for clinical use.\"}\n",
    "\n",
    "        dicom_seg_writer_pancreas = DICOMSegmentationWriterOperator(\n",
    "            self,\n",
    "            segment_descriptions=seg_descriptions_pancreas,\n",
    "            custom_tags=custom_tags_pancreas,\n",
    "            output_folder=app_output_path,\n",
    "            name=\"dicom_seg_writer_pancreas\",\n",
    "        )\n",
    "\n",
    "        # NOTE: Sharp eyed readers can already see that the above instantiation of object can be simply parameterized.\n",
    "        #       Very true, but leaving them as if for easy reading. In fact the whole app can be parameterized for general use.\n",
    "\n",
    "        # Create the processing pipeline, by specifying the upstream and downstream operators, and\n",
    "        # ensuring the output from the former matches the input of the latter, in both name and type.\n",
    "        self.add_flow(study_loader_op, series_selector_op, {(\"dicom_study_list\", \"dicom_study_list\")})\n",
    "        self.add_flow(\n",
    "            series_selector_op, series_to_vol_op, {(\"study_selected_series_list\", \"study_selected_series_list\")}\n",
    "        )\n",
    "\n",
    "        # Feed the input image to all inference operators\n",
    "        self.add_flow(series_to_vol_op, bundle_spleen_seg_op, {(\"image\", \"image\")})\n",
    "        # The Pancreas CT Seg bundle requires PyTorch 1.12.0 to avoid failure to load.\n",
    "        self.add_flow(series_to_vol_op, bundle_pancreas_seg_op, {(\"image\", \"image\")})\n",
    "\n",
    "        # Create DICOM Seg for one of the inference output\n",
    "        # Note below the dicom_seg_writer requires two inputs, each coming from a upstream operator.\n",
    "        self.add_flow(\n",
    "            series_selector_op, dicom_seg_writer_spleen, {(\"study_selected_series_list\", \"study_selected_series_list\")}\n",
    "        )\n",
    "        self.add_flow(bundle_spleen_seg_op, dicom_seg_writer_spleen, {(\"pred\", \"seg_image\")})\n",
    "\n",
    "        # Create DICOM Seg for one of the inference output\n",
    "        # Note below the dicom_seg_writer requires two inputs, each coming from a upstream operator.\n",
    "        self.add_flow(\n",
    "            series_selector_op,\n",
    "            dicom_seg_writer_pancreas,\n",
    "            {(\"study_selected_series_list\", \"study_selected_series_list\")},\n",
    "        )\n",
    "        self.add_flow(bundle_pancreas_seg_op, dicom_seg_writer_pancreas, {(\"pred\", \"seg_image\")})\n",
    "\n",
    "        logging.info(f\"End {self.compose.__name__}\")\n",
    "\n",
    "\n",
    "# This is a sample series selection rule in JSON, simply selecting CT series.\n",
    "# If the study has more than 1 CT series, then all of them will be selected.\n",
    "# Please see more detail in DICOMSeriesSelectorOperator.\n",
    "Sample_Rules_Text = \"\"\"\n",
    "{\n",
    "    \"selections\": [\n",
    "        {\n",
    "            \"name\": \"CT Series\",\n",
    "            \"conditions\": {\n",
    "                \"StudyDescription\": \"(.*?)\",\n",
    "                \"Modality\": \"(?i)CT\",\n",
    "                \"SeriesDescription\": \"(.*?)\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(f\"Begin {__name__}\")\n",
    "    App().run()\n",
    "    logging.info(f\"End {__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_app/__main__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_app/__main__.py\n",
    "import logging\n",
    "from app import App\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(f\"Begin {__name__}\")\n",
    "    App().run()\n",
    "    logging.info(f\"End {__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.py\t__main__.py  __pycache__\n"
     ]
    }
   ],
   "source": [
    "!ls my_app"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, let's execute the app on the command line. Note the required environment variables have been set with the specific input data and model paths from earlier steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-18 20:04:31,538 - Begin __main__\n",
      "2023-07-18 20:04:31,541 - Begin run\n",
      "2023-07-18 20:04:31,541 - Begin compose\n",
      "2023-07-18 20:04:31,557 - End compose\n",
      "[\u001b[32minfo\u001b[m] [gxf_executor.cpp:182] Creating context\n",
      "[\u001b[32minfo\u001b[m] [gxf_executor.cpp:1576] Loading extensions from configs...\n",
      "[\u001b[32minfo\u001b[m] [gxf_executor.cpp:1718] Activating Graph...\n",
      "[\u001b[32minfo\u001b[m] [gxf_executor.cpp:1748] Running Graph...\n",
      "[\u001b[32minfo\u001b[m] [gxf_executor.cpp:1750] Waiting for completion...\n",
      "[\u001b[32minfo\u001b[m] [gxf_executor.cpp:1751] Graph execution waiting. Fragment: \n",
      "[\u001b[32minfo\u001b[m] [greedy_scheduler.cpp:190] Scheduling 9 entities\n",
      "2023-07-18 20:04:31,687 - No or invalid input path from the optional input port: None\n",
      "2023-07-18 20:04:32,566 - Finding series for Selection named: CT Series\n",
      "2023-07-18 20:04:32,566 - Searching study, : 1.3.6.1.4.1.14519.5.2.1.7085.2626.822645453932810382886582736291\n",
      "  # of series: 1\n",
      "2023-07-18 20:04:32,566 - Working on series, instance UID: 1.3.6.1.4.1.14519.5.2.1.7085.2626.119403521930927333027265674239\n",
      "2023-07-18 20:04:32,566 - On attribute: 'StudyDescription' to match value: '(.*?)'\n",
      "2023-07-18 20:04:32,566 -     Series attribute StudyDescription value: CT ABDOMEN W IV CONTRAST\n",
      "2023-07-18 20:04:32,566 - Series attribute string value did not match. Try regEx.\n",
      "2023-07-18 20:04:32,567 - On attribute: 'Modality' to match value: '(?i)CT'\n",
      "2023-07-18 20:04:32,567 -     Series attribute Modality value: CT\n",
      "2023-07-18 20:04:32,567 - Series attribute string value did not match. Try regEx.\n",
      "2023-07-18 20:04:32,567 - On attribute: 'SeriesDescription' to match value: '(.*?)'\n",
      "2023-07-18 20:04:32,567 -     Series attribute SeriesDescription value: ABD/PANC 3.0 B31f\n",
      "2023-07-18 20:04:32,567 - Series attribute string value did not match. Try regEx.\n",
      "2023-07-18 20:04:32,567 - Selected Series, UID: 1.3.6.1.4.1.14519.5.2.1.7085.2626.119403521930927333027265674239\n",
      "2023-07-18 20:04:32,940 - Parsing from bundle_path: /home/mqin/src/monai-deploy-app-sdk/notebooks/tutorials/multi_models/pancreas_ct_dints/model.ts\n",
      "2023-07-18 20:06:09,612 - Parsing from bundle_path: /home/mqin/src/monai-deploy-app-sdk/notebooks/tutorials/multi_models/spleen_ct/model.ts\n",
      "/home/mqin/src/monai-deploy-app-sdk/.venv/lib/python3.8/site-packages/highdicom/valuerep.py:54: UserWarning: The string \"C3N-00198\" is unlikely to represent the intended person name since it contains only a single component. Construct a person name according to the format in described in http://dicom.nema.org/dicom/2013/output/chtml/part05/sect_6.2.html#sect_6.2.1.2, or, in pydicom 2.2.0 or later, use the pydicom.valuerep.PersonName.from_named_components() method to construct the person name correctly. If a single-component name is really intended, add a trailing caret character to disambiguate the name.\n",
      "  warnings.warn(\n",
      "2023-07-18 20:06:15,273 - add plane #0 for segment #1\n",
      "2023-07-18 20:06:15,274 - add plane #1 for segment #1\n",
      "2023-07-18 20:06:15,275 - add plane #2 for segment #1\n",
      "2023-07-18 20:06:15,276 - add plane #3 for segment #1\n",
      "2023-07-18 20:06:15,277 - add plane #4 for segment #1\n",
      "2023-07-18 20:06:15,278 - add plane #5 for segment #1\n",
      "2023-07-18 20:06:15,279 - add plane #6 for segment #1\n",
      "2023-07-18 20:06:15,280 - add plane #7 for segment #1\n",
      "2023-07-18 20:06:15,282 - add plane #8 for segment #1\n",
      "2023-07-18 20:06:15,283 - add plane #9 for segment #1\n",
      "2023-07-18 20:06:15,284 - add plane #10 for segment #1\n",
      "2023-07-18 20:06:15,285 - add plane #11 for segment #1\n",
      "2023-07-18 20:06:15,286 - add plane #12 for segment #1\n",
      "2023-07-18 20:06:15,288 - add plane #13 for segment #1\n",
      "2023-07-18 20:06:15,289 - add plane #14 for segment #1\n",
      "2023-07-18 20:06:15,290 - add plane #15 for segment #1\n",
      "2023-07-18 20:06:15,291 - add plane #16 for segment #1\n",
      "2023-07-18 20:06:15,292 - add plane #17 for segment #1\n",
      "2023-07-18 20:06:15,293 - add plane #18 for segment #1\n",
      "2023-07-18 20:06:15,294 - add plane #19 for segment #1\n",
      "2023-07-18 20:06:15,295 - add plane #20 for segment #1\n",
      "2023-07-18 20:06:15,296 - add plane #21 for segment #1\n",
      "2023-07-18 20:06:15,298 - add plane #22 for segment #1\n",
      "2023-07-18 20:06:15,299 - add plane #23 for segment #1\n",
      "2023-07-18 20:06:15,300 - add plane #24 for segment #1\n",
      "2023-07-18 20:06:15,301 - add plane #25 for segment #1\n",
      "2023-07-18 20:06:15,302 - add plane #26 for segment #1\n",
      "2023-07-18 20:06:15,303 - add plane #27 for segment #1\n",
      "2023-07-18 20:06:15,304 - add plane #28 for segment #1\n",
      "2023-07-18 20:06:15,305 - add plane #29 for segment #1\n",
      "2023-07-18 20:06:15,306 - add plane #30 for segment #1\n",
      "2023-07-18 20:06:15,307 - add plane #31 for segment #1\n",
      "2023-07-18 20:06:15,308 - add plane #32 for segment #1\n",
      "2023-07-18 20:06:15,310 - add plane #33 for segment #1\n",
      "2023-07-18 20:06:15,311 - add plane #34 for segment #1\n",
      "2023-07-18 20:06:15,312 - add plane #35 for segment #1\n",
      "2023-07-18 20:06:15,313 - add plane #36 for segment #1\n",
      "2023-07-18 20:06:15,314 - add plane #37 for segment #1\n",
      "2023-07-18 20:06:15,315 - add plane #38 for segment #1\n",
      "2023-07-18 20:06:15,316 - add plane #39 for segment #1\n",
      "2023-07-18 20:06:15,318 - add plane #40 for segment #1\n",
      "2023-07-18 20:06:15,319 - add plane #41 for segment #1\n",
      "2023-07-18 20:06:15,320 - add plane #42 for segment #1\n",
      "2023-07-18 20:06:15,321 - add plane #43 for segment #1\n",
      "2023-07-18 20:06:15,322 - add plane #44 for segment #1\n",
      "2023-07-18 20:06:15,323 - add plane #45 for segment #1\n",
      "2023-07-18 20:06:15,324 - add plane #46 for segment #1\n",
      "2023-07-18 20:06:15,325 - add plane #47 for segment #1\n",
      "2023-07-18 20:06:15,326 - add plane #48 for segment #1\n",
      "2023-07-18 20:06:15,327 - add plane #49 for segment #1\n",
      "2023-07-18 20:06:15,328 - add plane #50 for segment #1\n",
      "2023-07-18 20:06:15,329 - add plane #51 for segment #1\n",
      "2023-07-18 20:06:15,330 - add plane #52 for segment #1\n",
      "2023-07-18 20:06:15,331 - add plane #53 for segment #1\n",
      "2023-07-18 20:06:15,332 - add plane #54 for segment #1\n",
      "2023-07-18 20:06:15,333 - add plane #55 for segment #1\n",
      "2023-07-18 20:06:15,334 - add plane #56 for segment #1\n",
      "2023-07-18 20:06:15,335 - add plane #57 for segment #1\n",
      "2023-07-18 20:06:15,336 - add plane #58 for segment #1\n",
      "2023-07-18 20:06:15,337 - add plane #59 for segment #1\n",
      "2023-07-18 20:06:15,338 - add plane #60 for segment #1\n",
      "2023-07-18 20:06:15,339 - add plane #61 for segment #1\n",
      "2023-07-18 20:06:15,340 - add plane #62 for segment #1\n",
      "2023-07-18 20:06:15,341 - add plane #63 for segment #1\n",
      "2023-07-18 20:06:15,342 - add plane #64 for segment #1\n",
      "2023-07-18 20:06:15,343 - add plane #65 for segment #1\n",
      "2023-07-18 20:06:15,344 - add plane #66 for segment #1\n",
      "2023-07-18 20:06:15,345 - add plane #67 for segment #1\n",
      "2023-07-18 20:06:15,375 - skip empty plane 0 of segment #2\n",
      "2023-07-18 20:06:15,375 - skip empty plane 1 of segment #2\n",
      "2023-07-18 20:06:15,375 - skip empty plane 2 of segment #2\n",
      "2023-07-18 20:06:15,375 - skip empty plane 3 of segment #2\n",
      "2023-07-18 20:06:15,376 - skip empty plane 4 of segment #2\n",
      "2023-07-18 20:06:15,376 - skip empty plane 5 of segment #2\n",
      "2023-07-18 20:06:15,376 - skip empty plane 6 of segment #2\n",
      "2023-07-18 20:06:15,376 - skip empty plane 7 of segment #2\n",
      "2023-07-18 20:06:15,376 - skip empty plane 8 of segment #2\n",
      "2023-07-18 20:06:15,376 - skip empty plane 9 of segment #2\n",
      "2023-07-18 20:06:15,377 - skip empty plane 10 of segment #2\n",
      "2023-07-18 20:06:15,377 - skip empty plane 11 of segment #2\n",
      "2023-07-18 20:06:15,377 - skip empty plane 12 of segment #2\n",
      "2023-07-18 20:06:15,377 - skip empty plane 13 of segment #2\n",
      "2023-07-18 20:06:15,377 - skip empty plane 14 of segment #2\n",
      "2023-07-18 20:06:15,377 - skip empty plane 15 of segment #2\n",
      "2023-07-18 20:06:15,378 - skip empty plane 16 of segment #2\n",
      "2023-07-18 20:06:15,378 - skip empty plane 17 of segment #2\n",
      "2023-07-18 20:06:15,378 - skip empty plane 18 of segment #2\n",
      "2023-07-18 20:06:15,378 - skip empty plane 19 of segment #2\n",
      "2023-07-18 20:06:15,378 - skip empty plane 20 of segment #2\n",
      "2023-07-18 20:06:15,378 - skip empty plane 21 of segment #2\n",
      "2023-07-18 20:06:15,378 - skip empty plane 22 of segment #2\n",
      "2023-07-18 20:06:15,379 - skip empty plane 23 of segment #2\n",
      "2023-07-18 20:06:15,379 - skip empty plane 24 of segment #2\n",
      "2023-07-18 20:06:15,379 - skip empty plane 25 of segment #2\n",
      "2023-07-18 20:06:15,379 - skip empty plane 26 of segment #2\n",
      "2023-07-18 20:06:15,379 - skip empty plane 27 of segment #2\n",
      "2023-07-18 20:06:15,379 - skip empty plane 28 of segment #2\n",
      "2023-07-18 20:06:15,380 - skip empty plane 29 of segment #2\n",
      "2023-07-18 20:06:15,380 - skip empty plane 30 of segment #2\n",
      "2023-07-18 20:06:15,380 - skip empty plane 31 of segment #2\n",
      "2023-07-18 20:06:15,380 - skip empty plane 32 of segment #2\n",
      "2023-07-18 20:06:15,380 - skip empty plane 33 of segment #2\n",
      "2023-07-18 20:06:15,380 - skip empty plane 34 of segment #2\n",
      "2023-07-18 20:06:15,380 - skip empty plane 35 of segment #2\n",
      "2023-07-18 20:06:15,381 - skip empty plane 36 of segment #2\n",
      "2023-07-18 20:06:15,381 - skip empty plane 37 of segment #2\n",
      "2023-07-18 20:06:15,381 - skip empty plane 38 of segment #2\n",
      "2023-07-18 20:06:15,381 - skip empty plane 39 of segment #2\n",
      "2023-07-18 20:06:15,381 - skip empty plane 40 of segment #2\n",
      "2023-07-18 20:06:15,381 - skip empty plane 41 of segment #2\n",
      "2023-07-18 20:06:15,382 - skip empty plane 42 of segment #2\n",
      "2023-07-18 20:06:15,382 - skip empty plane 43 of segment #2\n",
      "2023-07-18 20:06:15,382 - skip empty plane 44 of segment #2\n",
      "2023-07-18 20:06:15,382 - skip empty plane 45 of segment #2\n",
      "2023-07-18 20:06:15,382 - skip empty plane 46 of segment #2\n",
      "2023-07-18 20:06:15,382 - skip empty plane 47 of segment #2\n",
      "2023-07-18 20:06:15,382 - skip empty plane 48 of segment #2\n",
      "2023-07-18 20:06:15,383 - skip empty plane 49 of segment #2\n",
      "2023-07-18 20:06:15,383 - skip empty plane 50 of segment #2\n",
      "2023-07-18 20:06:15,383 - skip empty plane 51 of segment #2\n",
      "2023-07-18 20:06:15,383 - skip empty plane 52 of segment #2\n",
      "2023-07-18 20:06:15,383 - skip empty plane 53 of segment #2\n",
      "2023-07-18 20:06:15,383 - skip empty plane 54 of segment #2\n",
      "2023-07-18 20:06:15,384 - skip empty plane 55 of segment #2\n",
      "2023-07-18 20:06:15,384 - skip empty plane 56 of segment #2\n",
      "2023-07-18 20:06:15,384 - skip empty plane 57 of segment #2\n",
      "2023-07-18 20:06:15,384 - skip empty plane 58 of segment #2\n",
      "2023-07-18 20:06:15,384 - skip empty plane 59 of segment #2\n",
      "2023-07-18 20:06:15,384 - skip empty plane 60 of segment #2\n",
      "2023-07-18 20:06:15,384 - skip empty plane 61 of segment #2\n",
      "2023-07-18 20:06:15,385 - skip empty plane 62 of segment #2\n",
      "2023-07-18 20:06:15,385 - skip empty plane 63 of segment #2\n",
      "2023-07-18 20:06:15,385 - skip empty plane 64 of segment #2\n",
      "2023-07-18 20:06:15,385 - skip empty plane 65 of segment #2\n",
      "2023-07-18 20:06:15,385 - skip empty plane 66 of segment #2\n",
      "2023-07-18 20:06:15,385 - skip empty plane 67 of segment #2\n",
      "2023-07-18 20:06:15,408 - copy Image-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:06:15,408 - copy attributes of module \"Specimen\"\n",
      "2023-07-18 20:06:15,408 - copy Patient-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:06:15,408 - copy attributes of module \"Patient\"\n",
      "2023-07-18 20:06:15,409 - copy attributes of module \"Clinical Trial Subject\"\n",
      "2023-07-18 20:06:15,409 - copy Study-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:06:15,409 - copy attributes of module \"General Study\"\n",
      "2023-07-18 20:06:15,409 - copy attributes of module \"Patient Study\"\n",
      "2023-07-18 20:06:15,409 - copy attributes of module \"Clinical Trial Study\"\n",
      "2023-07-18 20:06:16,665 - add plane #0 for segment #1\n",
      "2023-07-18 20:06:16,666 - add plane #1 for segment #1\n",
      "2023-07-18 20:06:16,667 - add plane #2 for segment #1\n",
      "2023-07-18 20:06:16,668 - add plane #3 for segment #1\n",
      "2023-07-18 20:06:16,669 - add plane #4 for segment #1\n",
      "2023-07-18 20:06:16,670 - add plane #5 for segment #1\n",
      "2023-07-18 20:06:16,671 - add plane #6 for segment #1\n",
      "2023-07-18 20:06:16,672 - add plane #7 for segment #1\n",
      "2023-07-18 20:06:16,672 - add plane #8 for segment #1\n",
      "2023-07-18 20:06:16,674 - add plane #9 for segment #1\n",
      "2023-07-18 20:06:16,674 - add plane #10 for segment #1\n",
      "2023-07-18 20:06:16,675 - add plane #11 for segment #1\n",
      "2023-07-18 20:06:16,676 - add plane #12 for segment #1\n",
      "2023-07-18 20:06:16,677 - add plane #13 for segment #1\n",
      "2023-07-18 20:06:16,678 - add plane #14 for segment #1\n",
      "2023-07-18 20:06:16,679 - add plane #15 for segment #1\n",
      "2023-07-18 20:06:16,680 - add plane #16 for segment #1\n",
      "2023-07-18 20:06:16,681 - add plane #17 for segment #1\n",
      "2023-07-18 20:06:16,682 - add plane #18 for segment #1\n",
      "2023-07-18 20:06:16,683 - add plane #19 for segment #1\n",
      "2023-07-18 20:06:16,684 - add plane #20 for segment #1\n",
      "2023-07-18 20:06:16,685 - add plane #21 for segment #1\n",
      "2023-07-18 20:06:16,686 - add plane #22 for segment #1\n",
      "2023-07-18 20:06:16,687 - add plane #23 for segment #1\n",
      "2023-07-18 20:06:16,688 - add plane #24 for segment #1\n",
      "2023-07-18 20:06:16,689 - add plane #25 for segment #1\n",
      "2023-07-18 20:06:16,690 - add plane #26 for segment #1\n",
      "2023-07-18 20:06:16,691 - add plane #27 for segment #1\n",
      "2023-07-18 20:06:16,692 - add plane #28 for segment #1\n",
      "2023-07-18 20:06:16,693 - add plane #29 for segment #1\n",
      "2023-07-18 20:06:16,694 - add plane #30 for segment #1\n",
      "2023-07-18 20:06:16,695 - add plane #31 for segment #1\n",
      "2023-07-18 20:06:16,696 - add plane #32 for segment #1\n",
      "2023-07-18 20:06:16,696 - add plane #33 for segment #1\n",
      "2023-07-18 20:06:16,697 - add plane #34 for segment #1\n",
      "2023-07-18 20:06:16,698 - add plane #35 for segment #1\n",
      "2023-07-18 20:06:16,699 - add plane #36 for segment #1\n",
      "2023-07-18 20:06:16,700 - add plane #37 for segment #1\n",
      "2023-07-18 20:06:16,701 - add plane #38 for segment #1\n",
      "2023-07-18 20:06:16,702 - add plane #39 for segment #1\n",
      "2023-07-18 20:06:16,703 - add plane #40 for segment #1\n",
      "2023-07-18 20:06:16,704 - add plane #41 for segment #1\n",
      "2023-07-18 20:06:16,705 - add plane #42 for segment #1\n",
      "2023-07-18 20:06:16,706 - add plane #43 for segment #1\n",
      "2023-07-18 20:06:16,707 - add plane #44 for segment #1\n",
      "2023-07-18 20:06:16,708 - add plane #45 for segment #1\n",
      "2023-07-18 20:06:16,709 - add plane #46 for segment #1\n",
      "2023-07-18 20:06:16,710 - add plane #47 for segment #1\n",
      "2023-07-18 20:06:16,711 - add plane #48 for segment #1\n",
      "2023-07-18 20:06:16,712 - add plane #49 for segment #1\n",
      "2023-07-18 20:06:16,713 - add plane #50 for segment #1\n",
      "2023-07-18 20:06:16,714 - add plane #51 for segment #1\n",
      "2023-07-18 20:06:16,715 - add plane #52 for segment #1\n",
      "2023-07-18 20:06:16,716 - add plane #53 for segment #1\n",
      "2023-07-18 20:06:16,717 - add plane #54 for segment #1\n",
      "2023-07-18 20:06:16,718 - add plane #55 for segment #1\n",
      "2023-07-18 20:06:16,719 - add plane #56 for segment #1\n",
      "2023-07-18 20:06:16,720 - add plane #57 for segment #1\n",
      "2023-07-18 20:06:16,721 - add plane #58 for segment #1\n",
      "2023-07-18 20:06:16,722 - add plane #59 for segment #1\n",
      "2023-07-18 20:06:16,723 - add plane #60 for segment #1\n",
      "2023-07-18 20:06:16,724 - add plane #61 for segment #1\n",
      "2023-07-18 20:06:16,725 - add plane #62 for segment #1\n",
      "2023-07-18 20:06:16,726 - add plane #63 for segment #1\n",
      "2023-07-18 20:06:16,727 - add plane #64 for segment #1\n",
      "2023-07-18 20:06:16,727 - add plane #65 for segment #1\n",
      "2023-07-18 20:06:16,728 - add plane #66 for segment #1\n",
      "2023-07-18 20:06:16,729 - add plane #67 for segment #1\n",
      "2023-07-18 20:06:16,730 - add plane #68 for segment #1\n",
      "2023-07-18 20:06:16,731 - add plane #69 for segment #1\n",
      "2023-07-18 20:06:16,732 - add plane #70 for segment #1\n",
      "2023-07-18 20:06:16,733 - add plane #71 for segment #1\n",
      "2023-07-18 20:06:16,734 - add plane #72 for segment #1\n",
      "2023-07-18 20:06:16,735 - add plane #73 for segment #1\n",
      "2023-07-18 20:06:16,736 - add plane #74 for segment #1\n",
      "2023-07-18 20:06:16,737 - add plane #75 for segment #1\n",
      "2023-07-18 20:06:16,738 - add plane #76 for segment #1\n",
      "2023-07-18 20:06:16,739 - add plane #77 for segment #1\n",
      "2023-07-18 20:06:16,740 - add plane #78 for segment #1\n",
      "2023-07-18 20:06:16,741 - add plane #79 for segment #1\n",
      "2023-07-18 20:06:16,742 - add plane #80 for segment #1\n",
      "2023-07-18 20:06:16,744 - add plane #81 for segment #1\n",
      "2023-07-18 20:06:16,745 - add plane #82 for segment #1\n",
      "2023-07-18 20:06:16,746 - add plane #83 for segment #1\n",
      "2023-07-18 20:06:16,747 - add plane #84 for segment #1\n",
      "2023-07-18 20:06:16,748 - add plane #85 for segment #1\n",
      "2023-07-18 20:06:16,749 - add plane #86 for segment #1\n",
      "2023-07-18 20:06:16,798 - copy Image-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:06:16,798 - copy attributes of module \"Specimen\"\n",
      "2023-07-18 20:06:16,798 - copy Patient-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:06:16,798 - copy attributes of module \"Patient\"\n",
      "2023-07-18 20:06:16,798 - copy attributes of module \"Clinical Trial Subject\"\n",
      "2023-07-18 20:06:16,798 - copy Study-related attributes from dataset \"1.3.6.1.4.1.14519.5.2.1.7085.2626.936983343951485811186213470191\"\n",
      "2023-07-18 20:06:16,798 - copy attributes of module \"General Study\"\n",
      "2023-07-18 20:06:16,798 - copy attributes of module \"Patient Study\"\n",
      "2023-07-18 20:06:16,799 - copy attributes of module \"Clinical Trial Study\"\n",
      "[\u001b[32minfo\u001b[m] [greedy_scheduler.cpp:369] Scheduler stopped: Some entities are waiting for execution, but there are no periodic or async entities to get out of the deadlock.\n",
      "[\u001b[32minfo\u001b[m] [greedy_scheduler.cpp:398] Scheduler finished.\n",
      "[\u001b[32minfo\u001b[m] [gxf_executor.cpp:1760] Graph execution deactivating. Fragment: \n",
      "[\u001b[32minfo\u001b[m] [gxf_executor.cpp:1761] Deactivating Graph...\n",
      "[\u001b[32minfo\u001b[m] [gxf_executor.cpp:1764] Graph execution finished. Fragment: \n",
      "2023-07-18 20:06:16,898 - End run\n",
      "2023-07-18 20:06:16,898 - End __main__\n"
     ]
    }
   ],
   "source": [
    "!rm -f output/*\n",
    "!python my_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.826.0.1.3680043.10.511.3.55625195093306273306908944877703034.dcm\n",
      "1.2.826.0.1.3680043.10.511.3.78928527547991998303106898773330275.dcm\n"
     ]
    }
   ],
   "source": [
    "!ls output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging app"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's package the app with [MONAI Application Packager](/developing_with_sdk/packaging_app)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pending completion of holoscan packager\n"
     ]
    }
   ],
   "source": [
    "!echo \"Pending completion of holoscan packager\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "Building a MONAI Application Package (Docker image) can take time. Use `-l DEBUG` option if you want to see the progress.\n",
    ":::\n",
    "\n",
    "We can see that the Docker image is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_app                                                                latest                                     4d3e2e280e9f   4 days ago      15GB\n"
     ]
    }
   ],
   "source": [
    "!docker image ls | grep my_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing packaged app locally\n",
    "\n",
    "The packaged app can be run locally through [MONAI Application Runner](/developing_with_sdk/executing_packaged_app_locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pending completion of Holoscan runner\n"
     ]
    }
   ],
   "source": [
    "!echo \"Pending completion of Holoscan runner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.826.0.1.3680043.10.511.3.55625195093306273306908944877703034.dcm\n",
      "1.2.826.0.1.3680043.10.511.3.78928527547991998303106898773330275.dcm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[error] [program.cpp:533] Attempted interrupting when not running (state=0).\n",
      "[error] [runtime.cpp:1400] Graph interrupt failed with error: GXF_INVALID_EXECUTION_SEQUENCE\n",
      "[error] [gxf_executor.cpp:1732] GxfGraphInterrupt Error: GXF_INVALID_EXECUTION_SEQUENCE\n",
      "[error] [gxf_executor.cpp:1733] Send interrupt once more to terminate immediately\n",
      "[error] [gxf_executor.cpp:1738] Interrupted by user (global signal handler)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!ls output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b4ab1155d0cd1042497eb40fd55b2d15caf4b3c0f9fbfcc7ba4404045d40f12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
